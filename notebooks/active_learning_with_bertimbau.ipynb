{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9ebb81-019c-472e-93d1-934989175430",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "    Improving Sentiment Prediction in the Pet Market <br> \n",
    "    with Active Learning and BERTimbau\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a5ea10-e62b-424b-8624-0e80bccaa776",
   "metadata": {},
   "source": [
    "*******************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3fabd-2d08-4fcd-a7e0-3b544e87d85b",
   "metadata": {},
   "source": [
    "<h2>1. Introduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc53b87-ee53-499d-a53b-22cbc885ef0d",
   "metadata": {},
   "source": [
    "In this project, we aim to build a sentiment analysis model for reviews collected from businesses in the **pet care sector** in Santo Andr√©, SP, using **Active Learning** and **BERTimbau**. The primary goal is to **predict the ratings** (from 1 to 5 stars) based on the sentiment expressed in the reviews. We leverage BERTimbau, a **transformer model** specifically trained on Brazilian Portuguese, to capture the nuances of the language used in the reviews.\n",
    "\n",
    "The dataset is **small** and highly **imbalanced**, with many reviews rated 1 and 5 stars, and fewer in the middle. Additionally, the rating provided by the customer often does not reflect the sentiment in the review, as the text tends to describe their experience in a **subjective** manner. For this reason, BERTimbau was chosen, as it is capable of capturing **complex sentiments**, enabling the model to better understand the full range of emotions expressed.\n",
    "\n",
    "To address the challenge of imbalanced data, we implement Active Learning, a technique that iteratively selects the most **uncertain examples** for manual labeling and model training. Starting with a small set of labeled data and progressively incorporating the most informative examples, the model is able to learn more efficiently and improve its performance over time.\n",
    "\n",
    "In conclusion, this project demonstrates that combining BERTimbau with Active Learning is an efficient approach for training sentiment analysis models with limited labeled data. By focusing on the most informative examples through Active Learning, the model iteratively improves and adapts, providing more accurate sentiment predictions. This approach allows businesses in the pet care sector to better interpret **customer feedback**, transforming subjective reviews into **reliable information** that can support **service improvements** and enhance customer engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39bbd88-50bd-4399-b04a-6e8a83c4a0e2",
   "metadata": {},
   "source": [
    "<h2>2. Initialization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a01a31b-1243-4b9a-a261-5dfa991d0672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    mean_absolute_error, \n",
    "    mean_squared_error\n",
    ")\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    AdamW\n",
    ")\n",
    "\n",
    "import accelerate\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cfccbd-c5f3-44ca-aeb4-b78ee0ac86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Pandas display: show all columns and suppress chained assignment warnings\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaede16-1fc7-4636-9c6e-fd350243ea48",
   "metadata": {},
   "source": [
    "<h2>3. Load the Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35233e43-1c84-4480-af9d-51c8884b81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.abspath(os.path.join(\"..\", \"data\", \"processed\", \"reviews_processed.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8dffcd-b214-41dc-b2a9-27653de82cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv(PATH, sep=\";\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832e3595-ae48-49c2-9c57-beb1f4714d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec211b-f96f-465d-a7ea-b956907cdab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7b4df-c102-4cc7-8ab2-32858da9d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68686941-501a-4402-a255-c2f10ac9aaad",
   "metadata": {},
   "source": [
    "<h2>4. Explore the Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36697b-fd6e-4513-b4f6-6525f97da634",
   "metadata": {},
   "source": [
    "<h3>4.1 Pre-analysis of the Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a769b0-816e-4fe7-a6ef-47d33c66ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the DataFrame (data types, non-null count, memory)\n",
    "reviews_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d173f6-18fc-44c6-af13-df2f5875abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns (mean, std, min, max, etc.)\n",
    "reviews_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763af25-83f3-4649-9537-ad7eb74f40b7",
   "metadata": {},
   "source": [
    "<h3>4.2 Distribution of Ratings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46a512-952a-41f3-8034-3a68538f69d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(fig, filename=\"rating_distribution.png\"):\n",
    "    \"\"\"\n",
    "    Saves the given figure to a file.\n",
    "\n",
    "    Parameters:\n",
    "    fig (matplotlib.figure.Figure): The figure to save.\n",
    "    filename (str): The name of the file to save the plot to (default is 'rating_distribution.png').\n",
    "    \"\"\"\n",
    "    # Save the figure to the specified file\n",
    "    fig.savefig(filename, bbox_inches='tight')  # Save with tight bounding box to avoid clipping\n",
    "\n",
    "    # Close the figure (so it doesn't display in the notebook)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a394bbc-1df8-4b75-9b71-088592bb9dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar(x, y):\n",
    "    \"\"\"\n",
    "    Creates a bar plot to visualize the distribution of ratings.\n",
    "\n",
    "    Parameters:\n",
    "    x (list or array-like): The categories for the x-axis (e.g., rating values).\n",
    "    y (list or array-like): The corresponding counts or frequencies for each category.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a figure and axis with a predefined size\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    # Define the width of the bars\n",
    "    bar_width = 0.4  \n",
    "\n",
    "    # Set the title and label for the x-axis\n",
    "    ax.set_title(\"Distribution of Ratings\", fontfamily='Arial Rounded MT Bold', fontsize=16, pad=15)\n",
    "    ax.set_xlabel(\"Rating\", fontsize=12)\n",
    "\n",
    "    # Add a grid for better readability (dashed lines, gray color, semi-transparent)\n",
    "    ax.grid(visible=True, linestyle='--', color='gray', alpha=0.7)\n",
    "\n",
    "    # Convert the x values to a NumPy array to ensure compatibility with plotting\n",
    "    x = np.array(x)  \n",
    "\n",
    "    # Create the bar plot with a custom color and border\n",
    "    bars = ax.bar(x, y, color=\"#ff6d0a\", edgecolor='#e6e6e6')\n",
    "\n",
    "    # Annotate each bar with its height (value)\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()  # Retrieve the height of the bar\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2, yval, round(yval, 2), \n",
    "            ha='center', va='bottom', color='black', fontsize=10\n",
    "        )  \n",
    "\n",
    "    # Set the x-axis tick labels based on the provided x values\n",
    "    ax.set_xticks(x)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Save the plot\n",
    "    save_plot(fig, os.path.join(\"..\", \"results\", \"figures\", \"rating_distribution.png\")) # Save as a PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d454aff-b1ff-4b6a-8d6b-e6d9981f635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar chart of rating distribution\n",
    "plot_bar(\n",
    "    reviews_df['Rating'].value_counts().sort_index().index,\n",
    "    reviews_df['Rating'].value_counts().sort_index().values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216dfbff-85ba-464f-bef5-583a522caacb",
   "metadata": {},
   "source": [
    "<h3>4.3 Distribution of Word Count</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff304619-0a45-4796-9e82-b0c3dd151166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(x):\n",
    "    \"\"\"\n",
    "    Plots a histogram showing the distribution of word count in reviews with a logarithmic y-axis.\n",
    "\n",
    "    Parameters:\n",
    "    - x (list or np.array): List or array of word counts from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Displays a histogram with improved styling and a log-scaled y-axis, and saves the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert input data to a NumPy array for better performance and compatibility\n",
    "    x = np.array(x)  \n",
    "\n",
    "    # Create a figure and axis with a specified size\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    # Plot the histogram with 100 bins, custom color, transparency, and black edges\n",
    "    ax.hist(x, bins=100, color=\"#1f77b4\", alpha=0.75, edgecolor=\"black\")\n",
    "\n",
    "    # Set the y-axis to a logarithmic scale for better visualization of frequency distribution\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "    # Set title and axis labels with appropriate fonts and sizes\n",
    "    ax.set_title(\"Distribution of Word Count in Reviews (Log Scale on Y)\", \n",
    "                 fontfamily=\"Arial Rounded MT Bold\", fontsize=16, pad=15)\n",
    "    ax.set_xlabel(\"Number of Words\", fontsize=12)\n",
    "    ax.set_ylabel(\"Frequency (log scale)\", fontsize=12)\n",
    "\n",
    "    # Add a dashed grid on the y-axis for better readability\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", color=\"gray\", alpha=0.6)\n",
    "\n",
    "    # Adjust layout to prevent overlapping elements\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the histogram\n",
    "    plt.show()\n",
    "\n",
    "    # Save the plot\n",
    "    save_plot(fig, os.path.join(\"..\", \"results\", \"figures\", \"word_count_distribution.png\")) # Save as a PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f527a-04c9-4ea9-bd08-518584f16d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of word count distribution\n",
    "plot_hist(reviews_df['Word Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083af943-4664-486d-b13a-b9bbfc0fc38d",
   "metadata": {},
   "source": [
    "<h3>4.4 Outliers in Word Count</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3a30e-ac15-4bf7-b51d-71766b85b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers(x):\n",
    "    \"\"\"\n",
    "    Plots a boxplot showing outliers in the distribution of word count in reviews,\n",
    "    with quartile values displayed in a legend instead of on the graph.\n",
    "\n",
    "    Parameters:\n",
    "    - x (list or np.array): List of word counts from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Displays a boxplot highlighting the outliers and quartile values in a legend.\n",
    "    \"\"\"\n",
    "    x = np.array(x)  # Convert input to numpy array for better performance\n",
    "\n",
    "    # Compute quartiles and median\n",
    "    Q1 = np.percentile(x, 25)  # First quartile (25%)\n",
    "    Q3 = np.percentile(x, 75)  # Third quartile (75%)\n",
    "    median = np.median(x)      # Median (50%)\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    # Plot boxplot\n",
    "    box = ax.boxplot(x, vert=False, patch_artist=True, \n",
    "                     boxprops=dict(facecolor=\"#87ceeb\", color=\"black\"), \n",
    "                     whiskerprops=dict(color=\"black\"),\n",
    "                     capprops=dict(color=\"black\"),\n",
    "                     medianprops=dict(color=\"#d62728\", linewidth=2),\n",
    "                     flierprops=dict(marker='o', markerfacecolor='#d62728', markersize=6, linestyle='none'))\n",
    "\n",
    "    # Titles and labels\n",
    "    ax.set_title(\"Outliers in Word Count Distribution\", fontfamily=\"Arial Rounded MT Bold\", fontsize=16, pad=15)\n",
    "    ax.set_xlabel(\"Number of Words\", fontsize=12)\n",
    "\n",
    "    # Grid styling\n",
    "    ax.grid(axis=\"x\", linestyle=\"--\", color=\"gray\", alpha=0.6)\n",
    "\n",
    "    # Create a legend to display quartile values\n",
    "    legend_text = f\"Q1: {Q1:.1f}\\nMedian: {median:.1f}\\nQ3: {Q3:.1f}\"\n",
    "    ax.legend([legend_text], loc=\"upper right\", fontsize=10, frameon=True, edgecolor=\"black\")\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the plot\n",
    "    save_plot(fig, os.path.join(\"..\", \"results\", \"figures\", \"word_count_outliers.png\")) # Save as a PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559b779-fd8c-4ab2-98af-0fa5ab33ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outliers in the 'Word Count' column\n",
    "plot_outliers(reviews_df['Word Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69805d8-81b4-49d0-8764-1ae33f3f108e",
   "metadata": {},
   "source": [
    "<h2>5. Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d99590b-29d8-4042-bd54-fa4736570101",
   "metadata": {},
   "source": [
    "<h3>5.1 Adding 'Predicted Rating' Column</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33797266-42f8-461b-b570-add62c42560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the \"Predicted Rating\" column exists; if not, insert it after the \"Rating\" column\n",
    "if \"Predicted Rating\" not in reviews_df.columns:\n",
    "    reviews_df.insert(reviews_df.columns.get_loc(\"Rating\") + 1, \"Predicted Rating\", pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed73fd92-cc62-4999-99af-4e686a231b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure \"Predicted Rating\" is of type Int64 (nullable integer)\n",
    "reviews_df[\"Predicted Rating\"] = reviews_df[\"Predicted Rating\"].astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e7f98-5e28-4cc0-9436-316e3e2b5c4a",
   "metadata": {},
   "source": [
    "<h3>5.2 Selecting Initial Labeled Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383d3b9-74aa-4e8a-976d-d189424189f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output path\n",
    "LABELED_DATA_PATH = os.path.join(\"..\", \"data\", \"active_learning\", \"initial_labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a28ba-30ca-43c9-bd5f-8bc5454f3ae5",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Run the following code only once at the beginning to select the initial labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f41bf-0ee6-47c9-ae9a-6c4e152add08",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Select an initial sample of 500 reviews for manual labeling  \n",
    "initial_labeled_data = reviews_df.sample(n=500, random_state=42)  \n",
    "\n",
    "# Save the selected data to a CSV file for manual labeling  \n",
    "initial_labeled_data.to_csv(LABELED_DATA_PATH, sep=\";\", index=False, encoding=\"utf-8\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d912523-7be4-4a92-8fae-7a7347f56f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_prediction_entry(file_path):\n",
    "    \"\"\"\n",
    "    Iterates through each row of the DataFrame, displaying the 'Text' and 'Rating' columns.\n",
    "    Allows the user to input a 'Predicted Rating' value, which is then stored in the DataFrame.\n",
    "    The file is updated after each entry to avoid data loss.\n",
    "    The user can exit by typing 'q' or 'exit'.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the CSV file containing the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the dataset from the specified CSV file\n",
    "    # Ensure 'Predicted Rating' is read as an integer, allowing missing values (NaN)\n",
    "    df = pd.read_csv(file_path, sep=\";\", encoding=\"utf-8\", dtype={\"Predicted Rating\": \"Int64\"})\n",
    "\n",
    "    # Get total number of reviews for progress tracking\n",
    "    total_reviews = len(df)\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for idx, (index, row) in enumerate(df.iterrows(), start=1):\n",
    "\n",
    "        # Skip rows where 'Predicted Rating' is already filled\n",
    "        if pd.notna(row[\"Predicted Rating\"]):\n",
    "            continue\n",
    "\n",
    "        # Clear the screen to improve readability\n",
    "        try:\n",
    "            clear_output(wait=False)  # Works in Jupyter Notebook\n",
    "        except NameError:\n",
    "            os.system(\"cls\" if os.name == \"nt\" else \"clear\")  # Works in Terminal\n",
    "\n",
    "        time.sleep(0.5)  # Small delay for better user experience\n",
    "        \n",
    "        # Display review information\n",
    "        print(f\"\\nReview {idx} of {total_reviews}\")  \n",
    "        print(f\"Review Text: {row['Text']}\")\n",
    "        print(f\"Actual Rating: {row['Rating']}\")\n",
    "        \n",
    "        # Loop until the user provides a valid input\n",
    "        while True:\n",
    "            user_input = input(\"Enter Predicted Rating (1-5) or type 'q' to exit: \").strip()\n",
    "\n",
    "            # Allow user to exit by typing 'q' or 'exit'\n",
    "            if user_input.lower() in [\"q\", \"exit\"]:\n",
    "                print(\"Exiting...\")\n",
    "                df.to_csv(file_path, index=False, sep=\";\", encoding=\"utf-8\")  # Save progress before exiting\n",
    "                return  # Stop execution\n",
    "\n",
    "            try:\n",
    "                predicted = int(user_input)  # Convert input to integer\n",
    "                if 1 <= predicted <= 5:  # Ensure rating is within the valid range\n",
    "                    df.at[index, \"Predicted Rating\"] = predicted\n",
    "                    break  # Exit the loop if input is valid\n",
    "                else:\n",
    "                    print(\"Invalid input! Please enter a number between 1 and 5.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input! Please enter a number or 'q' to exit.\")\n",
    "        \n",
    "        # Save updated DataFrame to CSV after each valid input to prevent data loss\n",
    "        df.to_csv(file_path, index=False, sep=\";\", encoding=\"utf-8\")\n",
    "        print(\"Data saved successfully!\")\n",
    "\n",
    "    print(\"\\nAll reviews have been processed.\")  # Message displayed when all reviews are labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37eb594-3bce-4e9b-a4bf-f70679ce3fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run manual prediction entry using LABELED_DATA_PATH.\n",
    "manual_prediction_entry(LABELED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab43c3f-c19e-485f-ba84-e8c286c8cca4",
   "metadata": {},
   "source": [
    "<h2>6. Sentiment Analysis Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90670843-b01a-48d4-90da-f1a02b0afa8f",
   "metadata": {},
   "source": [
    "<h3>6.1 Load BERTimbau Model and Tokenizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e0ab4-87a3-4fed-b1a9-1c17e9089206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model name and load the corresponding tokenizer\n",
    "MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\"  # Pretrained BERT model for Portuguese\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)  # Load the tokenizer for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a7a20f-0c91-4b6f-bc36-2f7318b6473f",
   "metadata": {},
   "source": [
    "<h3>6.2 Saving and Loading Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675bd099-2e73-4912-b455-899ebf8974fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save or load the intermediate model\n",
    "SAVED_MODEL_PATH = os.path.join(\"..\", \"models\", \"sentiment_model.pt\")  # Path for saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478cb501-a7df-4569-b058-3366667bd1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_path):\n",
    "    \"\"\" \n",
    "    Saves the model's state dictionary to a specified file path.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to be saved.\n",
    "        model_path (str): The path where the model will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Save the model's state dictionary\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c0d87-3cd5-49ae-9965-cf2be31e3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_path):\n",
    "    \"\"\" \n",
    "    Loads a pretrained model from a specified file path.\n",
    "\n",
    "    This function loads the model's state dictionary from the given path and sets the model to evaluation mode.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to be loaded.\n",
    "        model_path (str): The path to the saved model file.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The loaded model set to evaluation mode.\n",
    "    \"\"\"\n",
    "    # Load the model's state dictionary\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a5ebd-6b6e-48f7-b319-92e9fc938b6a",
   "metadata": {},
   "source": [
    "<h3>6.3 Saving and Loading Training State</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae6cb2-680f-4ddd-9bdd-8ad483683e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save or load the active learning state\n",
    "STATE_FILE = os.path.join(\"..\", \"data\", \"active_learning\", \"active_learning_state.json\")  # Path for saving the active learning state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f702a6-10b2-44e2-b788-7de6c9abb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state(labeled_texts, labeled_labels, remaining_indices, current_iteration, state_file=STATE_FILE):\n",
    "    \"\"\" \n",
    "    Saves the current state of the Active Learning process.\n",
    "\n",
    "    This function creates a dictionary containing the current labeled texts, labels, remaining indices, \n",
    "    and the current iteration. It then attempts to save this state as a JSON file.\n",
    "\n",
    "    Args:\n",
    "        labeled_texts (list): List of texts that have been labeled.\n",
    "        labeled_labels (list): Corresponding labels for the texts.\n",
    "        remaining_indices (list): Indices of the remaining unlabeled data.\n",
    "        current_iteration (int): The current iteration of the Active Learning loop.\n",
    "        state_file (str): Path to the file where the state will be saved (default is STATE_FILE).\n",
    "    \"\"\"\n",
    "    # Create a dictionary with the current state\n",
    "    state = {\n",
    "        \"labeled_texts\": labeled_texts,\n",
    "        \"labeled_labels\": labeled_labels,\n",
    "        \"remaining_indices\": remaining_indices,\n",
    "        \"current_iteration\": current_iteration\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Save the state as a JSON file\n",
    "        with open(state_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(state, f, ensure_ascii=False, indent=4)\n",
    "        print(\"State successfully saved!\")\n",
    "    except Exception as e:\n",
    "        # Handle errors during the saving process\n",
    "        print(f\"Error saving state: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e01b52-f9b0-41e9-af3f-a94e8958ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state():\n",
    "    \"\"\" \n",
    "    Loads the last saved state of the Active Learning process, if available.\n",
    "    \n",
    "    Checks if the state file exists, and loads its contents if so. Returns None if the file doesn't exist.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: The saved state as a dictionary, or None if no saved state exists.\n",
    "    \"\"\"\n",
    "    # Check if the state file exists\n",
    "    if os.path.exists(STATE_FILE):\n",
    "        # Open and load the JSON content of the state file\n",
    "        with open(STATE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    # Return None if the state file doesn't exist\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0e77a-c380-40d4-8e3b-46a32877c360",
   "metadata": {},
   "source": [
    "<h3>6.4 Loading Labeled Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8758f70-7e4a-4da7-8298-762edb8cb78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labeled_data():\n",
    "    \"\"\"\n",
    "    Loads labeled data from a CSV file.\n",
    "\n",
    "    If the labeled data file exists, it loads the data and returns the texts and ratings.\n",
    "    If the file does not exist, it prints a warning and returns empty lists.\n",
    "\n",
    "    Returns:\n",
    "        texts: List of text reviews.\n",
    "        labels: List of corresponding ratings (adjusted to range 0-4).\n",
    "    \"\"\"\n",
    "    if os.path.exists(LABELED_DATA_PATH):\n",
    "        df = pd.read_csv(LABELED_DATA_PATH, sep=\";\", encoding=\"utf-8\", dtype={\"Predicted Rating\": \"Int64\"})\n",
    "        \n",
    "        # Remove NaN values from labeled data\n",
    "        df = df.dropna(subset=[\"Predicted Rating\"])\n",
    "        \n",
    "        print(f\"{len(df)} labeled examples loaded.\")\n",
    "        \n",
    "        return df[\"Review ID\"].tolist(), df[\"Text\"].tolist(), df[\"Predicted Rating\"].astype(int).tolist()\n",
    "    else:\n",
    "        print(\"No labeled data found. Starting from scratch.\")\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f525548-79a2-4508-8c80-1c8924b8f565",
   "metadata": {},
   "source": [
    "<h3>6.5 Tokenization and Dataset Preparation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba44c3-6a01-4682-9663-738fe5e13d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for text reviews, prepares data for tokenization and model input.\n",
    "\n",
    "    Args:\n",
    "        texts: List of text reviews.\n",
    "        labels: List of labels corresponding to each review (optional).\n",
    "        tokenizer: Tokenizer used for text preprocessing (default: `tokenizer`).\n",
    "        max_length: Maximum length for the tokenized sequences (default: 512).\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels=None, tokenizer=tokenizer, max_length=512):\n",
    "        self.texts = texts  # Store the texts\n",
    "        self.labels = labels  # Store the labels (optional)\n",
    "        self.tokenizer = tokenizer  # Store the tokenizer\n",
    "        self.max_length = max_length  # Store max length for padding/truncating\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a single item (text and its label) and tokenizes it.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sample to fetch.\n",
    "\n",
    "        Returns:\n",
    "            item: Dictionary containing the tokenized input and its label (if available).\n",
    "        \"\"\"\n",
    "        # Tokenize the text at index `idx`\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"  # Return as PyTorch tensors\n",
    "        )\n",
    "        \n",
    "        # Squeeze the tensor to remove unnecessary dimensions\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        \n",
    "        # If labels are provided, add the label to the item\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return item  # Return the tokenized item (text and label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a64cea-394b-4741-abfa-ea29fb2fb928",
   "metadata": {},
   "source": [
    "<h3>6.6 Define Active Learning Strategy (Entropy-Based)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b8c42-84e9-41d5-84c2-02fce99437d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_entropy(model, dataset, batch_size=16, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Computes entropy for each example in the dataset using the model's predictions.\n",
    "    Entropy is used to measure uncertainty, helping to identify uncertain examples for Active Learning.\n",
    "\n",
    "    Args:\n",
    "        model: Pre-trained model used for predictions.\n",
    "        dataset: Dataset containing the text samples for which entropy is calculated.\n",
    "        batch_size: Number of samples per batch for processing.\n",
    "        device: The device for computation (e.g., 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        np.array: An array of entropy values for each sample.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)  # Create DataLoader for batching\n",
    "    entropy_values = []\n",
    "\n",
    "    # Loop through each batch in the dataset\n",
    "    for batch in dataloader:\n",
    "        # Move batch data to the specified device\n",
    "        batch = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "        outputs = model(**batch)  # Get model predictions\n",
    "        probs = F.softmax(outputs.logits, dim=-1)  # Apply softmax to get probabilities\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)  # Compute entropy (avoid log(0))\n",
    "        entropy_values.extend(entropy.cpu().numpy())  # Collect entropy values for each example\n",
    "\n",
    "        # Free memory\n",
    "        del batch, outputs, probs, entropy\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return np.array(entropy_values)  # Return entropy values for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d0de53-26a8-432d-b9c0-5a6ff8160435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_uncertain_examples(model, unlabeled_texts, n_samples=100, batch_size=16, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Selects the most uncertain examples based on entropy for Active Learning.\n",
    "\n",
    "    Args:\n",
    "        model: Pre-trained model used for predictions.\n",
    "        unlabeled_texts: List of texts that have not been labeled yet.\n",
    "        n_samples: Number of uncertain samples to select.\n",
    "        batch_size: Number of samples per batch for processing.\n",
    "        device: The device for computation (e.g., 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        np.array: Indices of the most uncertain examples in the dataset.\n",
    "    \"\"\"\n",
    "    dataset = ReviewDataset(unlabeled_texts, tokenizer=tokenizer)  # Prepare dataset\n",
    "    entropy_values = compute_entropy(model, dataset, batch_size=batch_size, device=device)  # Get entropy for each text\n",
    "    uncertain_indices = np.argsort(entropy_values)[-n_samples:]  # Select indices with highest entropy (uncertainty)\n",
    "    return uncertain_indices  # Retun indices of uncertain examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca09afaf-339f-4119-b59e-b3b12a14ff98",
   "metadata": {},
   "source": [
    "<h3>6.7 Fine-Tuning the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1345d5a-15b9-48e8-b3e5-6d594a08053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model, train_texts, train_labels, batch_size=16, epochs=3, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Fine-tunes the pre-trained model on the provided training data.\n",
    "\n",
    "    Args:\n",
    "        model: Pre-trained model to be fine-tuned.\n",
    "        train_texts: List of training texts.\n",
    "        train_labels: List of labels for training data.\n",
    "        batch_size: Number of samples per batch for training.\n",
    "        epochs: Number of epochs to train the model.\n",
    "        device: The device for computation (e.g., 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        model: The fine-tuned model.\n",
    "    \"\"\"\n",
    "    train_dataset = ReviewDataset(train_texts, train_labels)  # Prepare the training dataset\n",
    "\n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",  # Directory to save results\n",
    "        evaluation_strategy=\"no\",  # No evaluation during training\n",
    "        per_device_train_batch_size=batch_size,  # Batch size for training\n",
    "        num_train_epochs=epochs,  # Number of training epochs\n",
    "        save_strategy=\"no\",  # No saving during training\n",
    "        logging_dir=\"./logs\",  # Directory to save logs\n",
    "        logging_steps=10,  # Log every 10 steps\n",
    "        report_to=\"none\"  # Disable reporting\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset  # Dataset to train on\n",
    "    )\n",
    "\n",
    "    trainer.train()  # Start training\n",
    "    return model # Return the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a5c964-b89f-4c8d-b421-00bbcd11f725",
   "metadata": {},
   "source": [
    "<h3>6.8 Active Learning Loop</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0d9698-c419-4cb8-816e-a93e35909942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning_loop(reviews_df, iterations=20, samples_per_iteration=100, batch_size=16, device=\"cuda\", model_path=SAVED_MODEL_PATH):\n",
    "    \"\"\"\n",
    "    Run the Active Learning loop, selecting uncertain examples and fine-tuning the model iteratively.\n",
    "    After each iteration, manually label new examples, update the training set, and re-train the model.\n",
    "\n",
    "    Args:\n",
    "        reviews_df: DataFrame with the unlabeled reviews.\n",
    "        iterations: Number of Active Learning iterations.\n",
    "        samples_per_iteration: Number of uncertain examples to select each iteration.\n",
    "        batch_size: Number of examples per batch during fine-tuning.\n",
    "        device: The device for computation (e.g., 'cuda').\n",
    "        model_path: Path for saving and loading the model.\n",
    "\n",
    "    Returns:\n",
    "        model: The fine-tuned model after completing the Active Learning loop.\n",
    "    \"\"\"\n",
    "    _ , labeled_texts, labeled_labels = load_labeled_data()  # Load labeled data\n",
    "    labeled_labels = [label - 1 for label in labeled_labels]  # Adjust labels to range 0-4\n",
    "\n",
    "    saved_state = load_state()  # Check if there is a saved state to resume\n",
    "    if saved_state:\n",
    "        print(\"\\nResuming from last saved state...\")\n",
    "        labeled_texts = saved_state[\"labeled_texts\"]\n",
    "        labeled_labels = saved_state[\"labeled_labels\"]\n",
    "        remaining_indices = saved_state[\"remaining_indices\"]\n",
    "        start_iteration = saved_state[\"current_iteration\"] + 1\n",
    "    else:\n",
    "        remaining_indices = reviews_df.index.tolist()  # Start with all reviews\n",
    "        start_iteration = 0\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=5).to(device)\n",
    "\n",
    "    # If a saved model exists, load it\n",
    "    if os.path.exists(model_path):\n",
    "        model = load_model(model, model_path)\n",
    "    else:\n",
    "        print(\"No saved model found, starting training from scratch.\")\n",
    "\n",
    "    for i in range(start_iteration, iterations):\n",
    "        print(f\"\\nIteration {i+1}/{iterations}\")\n",
    "\n",
    "        # Fine-tune the model\n",
    "        model = fine_tune_model(model, labeled_texts, labeled_labels, batch_size=batch_size, device=device)\n",
    "\n",
    "        # Load or create labeled data CSV\n",
    "        if os.path.exists(LABELED_DATA_PATH):\n",
    "            labeled_df = pd.read_csv(LABELED_DATA_PATH, sep=\";\", encoding=\"utf-8\", dtype={\"Predicted Rating\": \"Int64\"})\n",
    "        else:\n",
    "            labeled_df = pd.DataFrame(columns=reviews_df.columns.tolist() + [\"Predicted Rating\"])\n",
    "\n",
    "        # Check how many examples from the last batch are still unlabeled\n",
    "        unlabeled_in_batch = labeled_df[labeled_df[\"Predicted Rating\"].isna()]\n",
    "\n",
    "        if len(unlabeled_in_batch) > 0:\n",
    "            print(f\"{len(unlabeled_in_batch)} examples still need labeling before selecting new ones.\")\n",
    "            manual_prediction_entry(LABELED_DATA_PATH)  # Ensure user completes labeling before proceeding\n",
    "            labeled_df = pd.read_csv(LABELED_DATA_PATH, sep=\";\", encoding=\"utf-8\", dtype={\"Predicted Rating\": \"Int64\"})\n",
    "\n",
    "        # Select new uncertain examples **only if all previous ones are labeled**\n",
    "        if labeled_df[\"Predicted Rating\"].isna().sum() == 0:\n",
    "            if remaining_indices:\n",
    "                # Select exactly 100 uncertain examples\n",
    "                uncertain_indices = select_uncertain_examples(\n",
    "                    model, reviews_df.loc[remaining_indices, \"Text\"].tolist(), samples_per_iteration, batch_size, device\n",
    "                )[:samples_per_iteration]  # Ensure we get exactly samples_per_iteration indices\n",
    "                \n",
    "                new_indices = [remaining_indices[idx] for idx in uncertain_indices]\n",
    "\n",
    "                # Avoid duplicates before adding\n",
    "                new_examples = reviews_df.loc[new_indices].copy()\n",
    "                new_examples[\"Predicted Rating\"] = None\n",
    "\n",
    "                existing_review_ids = set(labeled_df[\"Review ID\"].dropna().tolist())  # Avoid re-adding existing IDs\n",
    "                new_examples = new_examples[~new_examples[\"Review ID\"].isin(existing_review_ids)]\n",
    "\n",
    "                # Ensure exactly 100 examples (if possible)\n",
    "                if len(new_examples) > samples_per_iteration:\n",
    "                    new_examples = new_examples.sample(n=samples_per_iteration, random_state=42)\n",
    "\n",
    "                labeled_df = pd.concat([labeled_df, new_examples], ignore_index=True)\n",
    "                labeled_df.to_csv(LABELED_DATA_PATH, index=False, sep=\";\", encoding=\"utf-8\")\n",
    "\n",
    "                print(f\"Added {len(new_examples)} new examples for manual labeling.\")\n",
    "        \n",
    "        # Perform manual labeling\n",
    "        manual_prediction_entry(LABELED_DATA_PATH)\n",
    "\n",
    "        # Reload labeled data\n",
    "        labeled_df = pd.read_csv(LABELED_DATA_PATH, sep=\";\", encoding=\"utf-8\", dtype={\"Predicted Rating\": \"Int64\"})\n",
    "        newly_labeled = labeled_df.dropna(subset=[\"Predicted Rating\"])\n",
    "\n",
    "        labeled_texts.extend(newly_labeled[\"Text\"].tolist())\n",
    "        labeled_labels.extend([label - 1 for label in newly_labeled[\"Predicted Rating\"].tolist()])  # Ensure labels are in range 0-4\n",
    "\n",
    "        # Remove labeled examples from the remaining list\n",
    "        remaining_indices = [idx for idx in remaining_indices if idx not in newly_labeled.index]\n",
    "\n",
    "        # Save progress\n",
    "        save_state(labeled_texts, labeled_labels, remaining_indices, i)\n",
    "        print(\"Progress saved!\")\n",
    "\n",
    "        # Save the model after each iteration\n",
    "        save_model(model, model_path)\n",
    "\n",
    "        # Free memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\nActive Learning process completed!\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c60856a-a683-4b2b-814d-da1579974829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the active learning loop to train and update the model iteratively\n",
    "final_model = active_learning_loop(reviews_df, iterations=20, samples_per_iteration=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96a408-ada6-4e28-9fce-bbd6f3c6250c",
   "metadata": {},
   "source": [
    "<h2>7. Model Evaluation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10b047d-ca24-40b7-a58b-89a39e83ba9b",
   "metadata": {},
   "source": [
    "<h3>7.1 Load the Trained Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bdbfb6-9c23-4de1-9388-fae2aec9dad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_path, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained BERT model from the specified path and moves it to the specified device.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model weights.\n",
    "        device (str): Device to load the model on (\"cuda\" for GPU, \"cpu\" for CPU).\n",
    "    \n",
    "    Returns:\n",
    "        model (BertForSequenceClassification): The loaded BERT model ready for inference.\n",
    "    \"\"\"\n",
    "    model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=5)\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True, map_location=device))  # Load saved weights\n",
    "    model.to(device)  # Move the model to the specified device (GPU/CPU)\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout, batch norm)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7b5ce-2680-4e1e-8a85-e4ca9e200ebd",
   "metadata": {},
   "source": [
    "<h3>7.2 Load Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2549b4f-926d-4a74-90ac-2890487702f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled data\n",
    "_ , texts, labels = load_labeled_data()\n",
    "\n",
    "# Adjust labels (BERT expects labels to be 0-indexed)\n",
    "labels = [label - 1 for label in labels]\n",
    "\n",
    "# Convert texts and labels to numpy arrays\n",
    "texts = np.array(texts)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a754a81-112a-4c05-86f5-3b823d7dfcb8",
   "metadata": {},
   "source": [
    "<h3>7.3 Initialize Tokenizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4961157-df28-42a4-9bb0-405a7782b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = load_trained_model(SAVED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e245c7-4963-4d8d-a9d4-d9dd562d0b5c",
   "metadata": {},
   "source": [
    "<h3>7.4 Preprocess Texts</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a902619-8986-47f5-8b41-ffe038fe00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the texts\n",
    "def preprocess_texts(texts, tokenizer, max_length=256):\n",
    "    \"\"\"\n",
    "    Tokenizes and preprocesses input texts to be compatible with BERT's input format.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): A list of text samples to be tokenized.\n",
    "        tokenizer (BertTokenizer): A pre-trained BERT tokenizer.\n",
    "        max_length (int): The maximum length to pad/truncate the sequences to.\n",
    "    \n",
    "    Returns:\n",
    "        encodings (BatchEncoding): Tokenized inputs as PyTorch tensors.\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,  # Truncate texts that exceed max length\n",
    "        padding=True,  # Pad shorter texts\n",
    "        max_length=max_length,  # Ensure all texts are of max length\n",
    "        return_tensors=\"pt\"  # Return tensors for PyTorch\n",
    "    )\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7c2540-8a2d-46bf-8a8f-58a02954ce31",
   "metadata": {},
   "source": [
    "<h3>7.5 Stratified Shuffle Split (Train/Test Split)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd78b9-4874-49e6-aea8-e4c8b6858c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Stratified Shuffle Split to ensure balanced test set\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split data\n",
    "for _, test_index in sss.split(texts, labels):\n",
    "    X_test = [texts[i] for i in test_index]\n",
    "    y_test = [labels[i] for i in test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f2c73-afce-477d-90e8-767d039558c6",
   "metadata": {},
   "source": [
    "<h3>7.6 Model Prediction</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d4965-8dda-40c3-82e2-252ae3b53b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, texts, batch_size=16, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Makes predictions on the input texts using the trained BERT model.\n",
    "    \n",
    "    Args:\n",
    "        model (BertForSequenceClassification): The trained BERT model.\n",
    "        tokenizer (BertTokenizer): A pre-trained BERT tokenizer.\n",
    "        texts (list): A list of input texts to predict on.\n",
    "        batch_size (int): The batch size for processing the texts in smaller chunks.\n",
    "        device (str): Device to run the model on (\"cuda\" for GPU, \"cpu\" for CPU).\n",
    "    \n",
    "    Returns:\n",
    "        predictions (list): The list of predicted labels for the input texts.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    \n",
    "    # Iterate over the texts in batches\n",
    "    with torch.no_grad():  # No need to track gradients during inference\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            encodings = preprocess_texts(batch_texts, tokenizer).to(device)  # Preprocess and move to device\n",
    "            outputs = model(**encodings)  # Make prediction\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()  # Get predicted class (highest logit)\n",
    "            predictions.extend(preds)  # Collect predictions\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2489d-c6b4-4a7d-b28c-d4725fce94c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "pred_labels = predict(model, tokenizer, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d891dfd-d580-482c-963e-f20530d61b17",
   "metadata": {},
   "source": [
    "<h3>7.7 Evaluate the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7ebcf-811c-4487-b7a4-93e259a71324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "pred_labels = predict(model, tokenizer, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c91644-0fb8-4155-89f3-d616ebae4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, pred_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb64bd87-6024-4c4a-983d-29686f669ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift class labels from 0-4 to 1-5\n",
    "y_test_adjusted = np.array([y + 1 for y in y_test])\n",
    "pred_labels_adjusted = np.array([pred + 1 for pred in pred_labels])\n",
    "\n",
    "# Generate classification report with adjusted labels (1 to 5)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_adjusted, pred_labels_adjusted, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16152957-781d-48ff-afb0-06d3dffe2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title='Model Evaluation - Confusion Matrix', filename=\"model_evaluation_confusion_matrix.png\"):\n",
    "    \"\"\"\n",
    "    Creates a confusion matrix heatmap to visualize the model's performance.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): The true labels.\n",
    "    y_pred (array-like): The predicted labels.\n",
    "    title (str): The title of the plot. Default is 'Model Evaluation - Confusion Matrix'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Create a figure and axis with a predefined size\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "    # Plot the heatmap with annotations\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlOrBr\", \n",
    "                xticklabels=range(1, 6), yticklabels=range(1, 6),\n",
    "                linewidths=0.5, linecolor='lightgray', ax=ax)\n",
    "\n",
    "    # Set the title and labels\n",
    "    ax.set_title(title, fontfamily='Arial Rounded MT Bold', fontsize=16, pad=15)\n",
    "    ax.set_xlabel(\"Predicted\", fontsize=12)\n",
    "    ax.set_ylabel(\"Actual\", fontsize=12)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Save the plot\n",
    "    save_plot(fig, os.path.join(\"..\", \"results\", \"figures\", filename)) # Save as a PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f78be30-75a2-48f5-86b1-01a9edf807a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix to evaluate model performance\n",
    "plot_confusion_matrix(y_test, pred_labels, \"Model Evaluation - Confusion Matrix\", \"model_evaluation_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d2e25-e393-442a-9ab8-fc3a6d27bf7e",
   "metadata": {},
   "source": [
    "<h2>8. Predicting Ratings for Unlabeled Reviews</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4d5f4-129c-40ca-b126-d30f4d62659b",
   "metadata": {},
   "source": [
    "<h3>8.1 Load Labeled Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182d318-78b4-4aa8-be70-02d367df3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled data\n",
    "labeled_ids, labeled_texts, labeled_labels = load_labeled_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da79a9-2b22-49c5-a5c9-a0dcb9d56162",
   "metadata": {},
   "source": [
    "<h3>8.2 Map Predicted Ratings to Reviews</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de41f0c6-22bc-4d79-ada5-27f19b718611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping review IDs to predicted labels\n",
    "id_to_pred = {review_id: pred for review_id, pred in zip(labeled_ids, labeled_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23494999-bfa4-4f33-967d-c99e33cee281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the predicted ratings from labeled data to the reviews dataframe\n",
    "reviews_df[\"Predicted Rating\"] = reviews_df[\"Review ID\"].map(id_to_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe4aa8-fe9b-4ceb-a090-c02d7989ea57",
   "metadata": {},
   "source": [
    "<h3>8.3 Identify Unlabeled Reviews</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2bf129-f72c-4971-b2c2-4944f2f6c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get texts and IDs of unlabeled reviews\n",
    "unlabeled_reviews = reviews_df[reviews_df[\"Predicted Rating\"].isna()][\"Text\"].tolist()\n",
    "unlabeled_review_ids = reviews_df[reviews_df[\"Predicted Rating\"].isna()][\"Review ID\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee48268-a5d9-4331-8436-a1ed583ef347",
   "metadata": {},
   "source": [
    "<h3>8.4 Predict Ratings for Unlabeled Reviews</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9e59f-91e3-470a-9541-06ffeee6c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict ratings for unlabeled reviews\n",
    "unlabeled_predictions = predict(model, tokenizer, unlabeled_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce5ae9-5ff8-47e1-845a-49bd058cd3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the predictions to be on a scale of 1-5\n",
    "unlabeled_predictions = [pred + 1 for pred in unlabeled_predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a094455e-3a0f-491c-91e2-93070a0d3e4a",
   "metadata": {},
   "source": [
    "<h3>8.5 Map Predicted Ratings to Unlabeled Reviews</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ae94b7-a343-4c38-979a-21826bb9b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for unlabeled review predictions\n",
    "id_to_pred_unlabeled = {review_id: pred for review_id, pred in zip(unlabeled_review_ids, unlabeled_predictions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0dba86-3c8a-4830-b529-97f3e287b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the predicted ratings to the reviews dataframe\n",
    "reviews_df.loc[reviews_df[\"Review ID\"].isin(id_to_pred_unlabeled.keys()), \"Predicted Rating\"] = \\\n",
    "    reviews_df[\"Review ID\"].map(id_to_pred_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1dcf18-7ea5-4472-aa15-0eec8194cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'Predicted Rating' column is of type Int64 (with support for NaN values)\n",
    "reviews_df[\"Predicted Rating\"] = reviews_df[\"Predicted Rating\"].astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d8b9b-7599-4d1c-9805-2887df5e0264",
   "metadata": {},
   "source": [
    "<h3>8.6 Save the Results to a CSV File</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c8095-d549-467d-be66-7427fe70b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for saving the CSV file\n",
    "REVIEW_PATH = os.path.join(\"..\", \"results\", \"predictions\", \"reviews_with_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f7e4b-9c77-46a3-8d46-cd96e4b57003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a CSV file\n",
    "reviews_df.to_csv(REVIEW_PATH, index=False, sep=\";\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1b38f9-0b3d-4994-b705-6f1d25eb6957",
   "metadata": {},
   "source": [
    "<h2>9. Comparative Analysis: Actual vs. Predicted Ratings</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b6a0a-860d-43a8-8fa7-2ad058bbba4c",
   "metadata": {},
   "source": [
    "<h3>9.1 Distribution of Actual and Predicted Ratings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7c591-cb97-4901-98f5-348edd92779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rating_comparison(reviews_df):\n",
    "    \"\"\"\n",
    "    Creates a histogram to compare the distribution of actual ratings vs. predicted ratings.\n",
    "\n",
    "    Parameters:\n",
    "    reviews_df (DataFrame): A DataFrame containing the 'Rating' and 'Predicted Rating' columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define figure size for the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))  \n",
    "\n",
    "    # Plot the distribution of actual ratings with KDE\n",
    "    sns.histplot(reviews_df[\"Rating\"].dropna(), bins=5, kde=True, color=\"#1f77b4\", label=\"Actual Rating\", \n",
    "                 alpha=0.6, discrete=True, ax=ax)\n",
    "\n",
    "    # Plot the distribution of predicted ratings with KDE\n",
    "    sns.histplot(reviews_df[\"Predicted Rating\"].dropna(), bins=5, kde=True, color=\"#ff7f0e\", label=\"Predicted Rating\", \n",
    "                 alpha=0.6, discrete=True, ax=ax)\n",
    "\n",
    "    # Set labels and title for the axes\n",
    "    ax.set_xlabel(\"Rating\", fontsize=12)\n",
    "    ax.set_ylabel(\"Count\", fontsize=12)\n",
    "    ax.set_title(\"Distribution of Actual vs. Predicted Ratings\", fontfamily='Arial Rounded MT Bold', fontsize=16, pad=15)\n",
    "\n",
    "    # Set x-axis ticks to represent the possible rating values (1 to 5)\n",
    "    ax.set_xticks(range(1, 6))\n",
    "\n",
    "    # Add a grid for better readability (dashed lines, gray color, semi-transparent)\n",
    "    ax.grid(visible=True, linestyle='--', color='gray', alpha=0.7)\n",
    "\n",
    "    # Display legend to distinguish the actual and predicted ratings\n",
    "    ax.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Save the plot\n",
    "    save_plot(fig, os.path.join(\"..\", \"results\", \"figures\", \"actual_vs_predicted_ratings.png\")) # Save as a PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683dc9d5-5008-4b40-a49c-24d23a5eca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of actual vs predicted ratings\n",
    "plot_rating_comparison(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d717d9-3063-4b6c-bfce-fc7baa70f52e",
   "metadata": {},
   "source": [
    "<h3>9.2 Performance Metrics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b9bdb2-bde9-4e2f-b601-6c9c024b3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "accuracy = accuracy_score(reviews_df[\"Rating\"], reviews_df[\"Predicted Rating\"])\n",
    "mae = mean_absolute_error(reviews_df[\"Rating\"], reviews_df[\"Predicted Rating\"])\n",
    "rmse = np.sqrt(mean_squared_error(reviews_df[\"Rating\"], reviews_df[\"Predicted Rating\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455647ec-0924-4060-9216-7a55b10c1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f8834-a135-4434-9396-b01c443a8d4a",
   "metadata": {},
   "source": [
    "<h3>9.3 Confusion Matrix</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee971045-cb36-43b9-9bf5-766b58cd8db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN values for comparison\n",
    "valid_df = reviews_df.dropna(subset=[\"Rating\", \"Predicted Rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87749585-9b9a-4827-ac79-ca47f28539d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(valid_df[\"Rating\"], \n",
    "                      valid_df[\"Predicted Rating\"], \n",
    "                      \"Prediction vs Actual - Confusion Matrix\", \n",
    "                      \"prediction_vs_actual_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76e4c3-2d85-4dde-9d02-6003383f9449",
   "metadata": {},
   "source": [
    "<h3>9.4 Error Analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8bfab-d253-465c-9a09-de9d1b753ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute absolute errors\n",
    "valid_df[\"Error\"] = abs(valid_df[\"Rating\"] - valid_df[\"Predicted Rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347d24b-ffeb-4648-bb14-34eb5ef5c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_error(valid_df):\n",
    "    \"\"\"\n",
    "    Creates a bar plot to visualize the mean absolute error by rating category,\n",
    "    displaying the values above the bars with a background grid.\n",
    "\n",
    "    Parameters:\n",
    "    valid_df (DataFrame): A DataFrame containing the 'Rating' and 'Error' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define figure size\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    # Ensure the style does not override grid settings\n",
    "    sns.set_style(\"whitegrid\")  # Light background with grid\n",
    "\n",
    "    # Plot the bar plot with the coolwarm color palette\n",
    "    barplot = sns.barplot(x=\"Rating\", y=\"Error\", data=valid_df, hue=\"Rating\", \n",
    "                          palette=\"rocket\", legend=False, ax=ax)\n",
    "\n",
    "    # Set the labels and title\n",
    "    ax.set_xlabel(\"Actual Rating\", fontsize=12)\n",
    "    ax.set_ylabel(\"Mean Absolute Error\", fontsize=12)\n",
    "    ax.set_title(\"Average Error by Rating Category\", fontsize=14, fontfamily='Arial Rounded MT Bold', pad=15)\n",
    "\n",
    "    # Force grid appearance\n",
    "    ax.grid(True, linestyle='--', alpha=0.7, color='gray')  # Dashed light-gray grid\n",
    "    ax.set_axisbelow(True)  # Ensures the grid is behind the bars\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Save the plot\n",
    "    save_plot(fig, os.path.join(\"..\", \"results\", \"figures\", \"average_error_by_rating_category.png\")) # Save as a PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ffb4f-b80c-41cc-8b32-55fed1a21a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your data (valid_df)\n",
    "plot_average_error(valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc672de-d3ae-48d2-872c-b5fd91bd8655",
   "metadata": {},
   "source": [
    "<h3>9.5 Correct and Incorrect Predictions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e6e47-9931-422f-b364-813ddd4388fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct predictions\n",
    "correct_preds = valid_df[valid_df[\"Rating\"] == valid_df[\"Predicted Rating\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642dfdaa-65b7-460b-b921-ee2dd80af568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying 5 examples of correct predictions\n",
    "print(\"Examples of Correct Predictions:\")\n",
    "correct_preds[[\"Text\", \"Rating\", \"Predicted Rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7398232-49e0-486d-942d-8a709ae2aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorrect predictions\n",
    "incorrect_preds = valid_df[valid_df[\"Rating\"] != valid_df[\"Predicted Rating\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1c36a-827d-49df-91d6-542ee8eaea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying 5 examples of incorrect predictions\n",
    "print(\"Examples of Incorrect Predictions:\")\n",
    "incorrect_preds[[\"Text\", \"Rating\", \"Predicted Rating\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297165d4-83bf-40a3-84df-2e4b11f4ce38",
   "metadata": {},
   "source": [
    "<h2>10. Conclusion</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2118ad-d1e0-438e-8b88-9240ff0b916d",
   "metadata": {},
   "source": [
    "### **Model Performance Overview**  \n",
    "\n",
    "The model has achieved an exceptional **accuracy of 99.54%**, demonstrating strong performance across all sentiment classes. Analyzing the detailed metrics:  \n",
    "\n",
    "- **Precision remains close to 1.0 across all classes**, indicating minimal false positives.  \n",
    "- **Recall is consistently high**, with perfect recall (1.0) for classes **2, 4, and 5**, meaning that all actual instances in these classes were correctly identified.  \n",
    "- **The Macro F1-score is 0.9910**, showing that the model maintains a balanced performance across all classes, despite the dataset being highly imbalanced.  \n",
    "- **The Weighted F1-score is 0.9954**, confirming that the model performs well across all classes while accounting for class imbalance. This suggests that the model does not disproportionately favor majority classes but maintains strong predictive accuracy even for underrepresented ones.  \n",
    "\n",
    "These results highlight the effectiveness of combining **BERTimbau with Active Learning** for sentiment analysis, especially when dealing with limited and imbalanced labeled data. The model demonstrates the ability to generalize well across different sentiment levels, ensuring reliable predictions for both majority and minority classes.  \n",
    "\n",
    "### **Comparison with Actual Ratings**  \n",
    "\n",
    "When comparing the actual ratings provided by customers with the predicted ratings, the results are as follows:  \n",
    "\n",
    "- **Accuracy: 94.77%**, indicating that the predicted ratings align well with customer evaluations.  \n",
    "- **Mean Absolute Error (MAE): 0.0690**, demonstrating that the model‚Äôs predictions have a small average deviation from the true ratings.  \n",
    "- **Root Mean Squared Error (RMSE): 0.3380**, showing that the model maintains consistently accurate predictions with minimal discrepancies.  \n",
    "\n",
    "Despite the overall strong performance, **larger errors were observed in intermediate classes (2, 3, and 4).** These ratings often reflect more nuanced experiences and can be more subjective, making them inherently harder to classify. This suggests that additional labeled data and further fine-tuning may be necessary to improve the model‚Äôs ability to differentiate these sentiment levels. One possible approach is to leverage Active Learning to prioritize the labeling of uncertain cases in these classes, helping refine the model‚Äôs understanding of subtle differences in sentiment.  \n",
    "\n",
    "### **Final Thoughts**  \n",
    "\n",
    "These findings confirm that **BERTimbau combined with Active Learning** is a highly effective approach for sentiment analysis in customer reviews. The model not only captures the sentiment nuances across different rating levels but also demonstrates **robust generalization**, even in a **highly imbalanced dataset**. This makes it a valuable tool for **understanding customer feedback in the pet care sector**, providing insights that can support business decisions and service improvements.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e1cad-3ef5-4683-b3eb-a3d5327656a9",
   "metadata": {},
   "source": [
    "<h2>11. Next Steps</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f61b7b-db2f-44f3-98d2-2af073a37378",
   "metadata": {},
   "source": [
    "While the model has demonstrated **excellent performance**, there are several areas that can be explored further to improve its accuracy and applicability. Some potential next steps include:  \n",
    "\n",
    "### **Fine-Tune Model Performance**  \n",
    "- **Explore Hyperparameter Tuning:** Adjusting hyperparameters such as learning rate, batch size, and the number of layers could help optimize performance, particularly in improving precision and recall for specific classes.  \n",
    "- **Test with Different Models:** While BERTimbau has shown strong results, experimenting with other transformer models like **DistilBERT, RoBERTa, or domain-specific transformers** could reveal alternative solutions that improve computational efficiency or predictive accuracy.  \n",
    "- **Compare with Simpler Models:** Evaluating **Logistic Regression, Random Forest, or SVM** as baselines might offer insights into whether simpler models can achieve competitive performance with less computational cost.  \n",
    "\n",
    "### **Enhance Active Learning Strategies**  \n",
    "- **Evaluate Training and Validation Curves:** Analyzing the model's performance over time as more labeled data is added can help detect signs of **overfitting or underfitting**. Plotting these curves will provide insights into when additional training data brings diminishing returns and when to stop the Active Learning process.  \n",
    "- **Experiment with Different Active Learning Strategies:** While entropy-based sampling has been effective, testing **least confidence, margin sampling, or other uncertainty-based approaches** could refine the model‚Äôs ability to select the most informative examples for labeling.  \n",
    "- **Assess the Impact of Additional Labeled Data:** Monitoring how accuracy, precision, and recall evolve with each batch of newly labeled data will help determine whether collecting more labels continues to enhance the model‚Äôs performance or if the gains plateau.  \n",
    "\n",
    "By exploring these next steps, the model can be further refined to **deliver even more precise and actionable predictions**. This will enable businesses in the **pet care sector** to better understand and respond to customer feedback, improving their services based on deeper sentiment insights.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
