{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4366f943-d1e8-4b21-88cc-251bf8db2e50",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "    Topic Modeling of Pet Service Reviews <br> Using BERTimbau and HDBSCAN\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb824f-abee-48a2-811f-7d7aa4ac13fe",
   "metadata": {},
   "source": [
    "*******************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e83516-cff9-497d-a61e-2e5e08060234",
   "metadata": {},
   "source": [
    "<h2>1. Introduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50c8f0-d18e-453b-a21d-752f6fd8559c",
   "metadata": {},
   "source": [
    "This project focuses on **topic modeling** of customer reviews written in **Brazilian Portuguese**, specifically from pet-related businesses located in **Santo André/SP, Brazil**. Online reviews often contain informal, context-dependent language, which presents a challenge for extracting meaningful themes — particularly in languages other than English.\n",
    "\n",
    "To tackle this challenge, we designed a modern NLP pipeline centered around **BERTimbau**, a transformer-based language model pre-trained on Brazilian Portuguese. BERTimbau was used to generate **dense semantic embeddings** that capture the underlying meaning and nuance of each review, going beyond simple keyword matching.\n",
    "\n",
    "To identify coherent thematic structures within the review data, we applied **HDBSCAN**, a clustering algorithm well-suited for high-dimensional, text-based embeddings. HDBSCAN was chosen because it **automatically detects the number of clusters** based on the semantic density of the data, adapting to areas with varying complexity. This made it a flexible and robust choice for modeling the diversity of customer opinions.\n",
    "\n",
    "After clustering, **BERTimbau** was used again — this time to extract the most relevant keywords from each cluster. Its strong pretraining on Portuguese helped highlight key terms that accurately reflect the core content of each group, especially when dealing with subtle or informal language.\n",
    "\n",
    "To make the topics easier to interpret, we then applied **KeyBERT**, a keyword extraction library that selects representative terms by comparing them with the original embeddings. This step enabled us to generate concise, human-readable labels that summarize each cluster's central idea.\n",
    "\n",
    "For clearer insights, the reviews were divided into *positive* and *negative* categories based on their star ratings. Topic modeling was performed separately on each group, allowing us to compare recurring themes in positive versus negative feedback, and observe which aspects customers tend to praise or criticize most.\n",
    "\n",
    "This pipeline — combining **contextual embeddings**, **adaptive clustering**, and **keyword summarization** — allowed us to highlight common concerns, expectations, and experiences shared by customers in their own words, providing a detailed view of feedback within the pet care sector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bec24a-9e06-47cc-82ea-edfac3a2e396",
   "metadata": {},
   "source": [
    "<h2>2. Initialization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7abd10a-cb6b-46eb-a6d3-6e1dcc89c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "import langid\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b16d4-f791-4135-9ce2-7ae581dae83d",
   "metadata": {},
   "source": [
    "<h2>3. Load the Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b352b21-cf45-49b2-b78a-0dadd25240e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.abspath(os.path.join(\"..\", \"results\", \"predictions\", \"reviews_with_predictions.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cba518-d81c-4fed-a763-801a0ef5085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv(PATH, sep=\";\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee3689-1481-48c2-ac5d-2731791a7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f08e2f-d151-4475-8273-867a310c06cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d71a7c-3fb9-4ea5-95a4-5e13ee4e9960",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f6747-6a6f-4347-801e-80a805e4a2e3",
   "metadata": {},
   "source": [
    "<h2>4. Data Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d4013-b254-4672-88dc-25e1195b7c7f",
   "metadata": {},
   "source": [
    "<h3>4.1 Filtering Out Short Reviews</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f73ac5-4c66-47cf-82ef-bab8567500bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_reviews_by_min_words(df, text_column=\"Text\", min_words=4):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame to keep only reviews with at least a specified number of words.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the reviews.\n",
    "        text_column (str): The name of the column containing the review texts.\n",
    "        min_words (int): Minimum number of words required for a review to be kept. Default is 4.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered DataFrame with only reviews that meet the minimum word count.\n",
    "    \"\"\"\n",
    "    return df[df[text_column].apply(lambda x: isinstance(x, str) and len(x.strip().split()) >= min_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf11b6-1a53-402e-a6a6-894792e56e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to keep only reviews with at least 4 words\n",
    "reviews_df = filter_reviews_by_min_words(reviews_df, text_column=\"Text\", min_words=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893d479-1473-43e1-b2a3-24b5fbd8c3b3",
   "metadata": {},
   "source": [
    "<h3>4.2 Excluding Reviews in English</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c286de7c-b1a4-4715-89ca-56776420eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    \"\"\"\n",
    "    Detects whether a given text is in English using langid.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the text is detected as English, False otherwise.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return False\n",
    "    lang, _ = langid.classify(text)\n",
    "    return lang == \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ca165-ce05-41bd-86a8-ae0b08aa2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the review text is in English\n",
    "reviews_df = reviews_df[~reviews_df[\"Text\"].apply(is_english)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66728c-19ff-4a79-a98f-1009be9c647f",
   "metadata": {},
   "source": [
    "<h3>4.3 Categorizing Ratings into Sentiment Labels</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0027a04-61d8-4b82-a51f-ee966d92bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize ratings into \"Negative\", \"Positive\", and \"Neutral\" sentiments\n",
    "reviews_df[\"Sentiment\"] = reviews_df[\"Predicted Rating\"].map(\n",
    "    lambda x: \"Negative\" if x in [1, 2] \n",
    "    else \"Positive\" if x in [4, 5] \n",
    "    else \"Neutral\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d35a206-1769-4d21-a152-b942859297b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate texts\n",
    "positive_reviews = reviews_df[reviews_df[\"Sentiment\"] == \"Positive\"][\"Text\"].tolist()\n",
    "negative_reviews = reviews_df[reviews_df[\"Sentiment\"] == \"Negative\"][\"Text\"].tolist()\n",
    "neutral_reviews = reviews_df[reviews_df[\"Sentiment\"] == \"Neutral\"][\"Text\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d42cde5-ae2a-40ce-89d9-27756221c81c",
   "metadata": {},
   "source": [
    "<h3>4.4 Distribution of Sentiments</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1848cf-2f89-46f0-b5eb-22cfe467ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(fig, filename=\"sentiment_distribution.png\"):\n",
    "    \"\"\"\n",
    "    Saves the given figure to a file.\n",
    "\n",
    "    Args:\n",
    "        fig (matplotlib.figure.Figure): The figure to save.\n",
    "        filename (str): The name of the file to save the plot to. Default is 'sentiment_distribution.png'.\n",
    "    \"\"\"\n",
    "    # Save the figure to the specified file\n",
    "    fig.savefig(filename, bbox_inches='tight')  # Save with tight bounding box to avoid clipping\n",
    "\n",
    "    # Close the figure (so it doesn't display in the notebook)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb951c8-d632-4991-a3e1-284bdf2eedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot(x, y):\n",
    "    \"\"\"\n",
    "    Creates a bar plot to visualize the distribution of sentiments.\n",
    "\n",
    "    Args:\n",
    "        x (list or array-like): The categories for the x-axis (e.g., sentiment labels).\n",
    "        y (list or array-like): The corresponding counts or frequencies for each category.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create a figure and axis with a predefined size\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    \n",
    "    # Define the width of the bars\n",
    "    bar_width = 0.85 \n",
    "\n",
    "    # Set the title and label for the x-axis\n",
    "    ax.set_title(\"Distribution of Sentiments\", fontfamily='Arial Rounded MT Bold', fontsize=16, pad=15)\n",
    "    ax.set_xlabel(\"Sentiment\", fontsize=12)\n",
    "\n",
    "    # Add a grid for better readability (dashed lines, gray color, semi-transparent)\n",
    "    ax.grid(visible=True, linestyle='--', color='gray', alpha=0.7)\n",
    "\n",
    "    # Convert the x values to a NumPy array to ensure compatibility with plotting\n",
    "    x = np.array(x)  \n",
    "\n",
    "    # Create the bar plot with a custom color and border\n",
    "    bars = ax.bar(x, y, width=bar_width, color=\"#3498db\", edgecolor='#e6e6e6')\n",
    "\n",
    "    # Annotate each bar with its height (value)\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()  # Retrieve the height of the bar\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2, yval, round(yval, 2), \n",
    "            ha='center', va='bottom', color='black', fontsize=10\n",
    "        )  \n",
    "\n",
    "    # Set the x-axis tick labels based on the provided x values\n",
    "    ax.set_xticks(x)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Save the plot\n",
    "    save_plot(fig, os.path.join(\"..\", \"results\", \"figures\", \"topic_modeling\", \"sentiment_distribution.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b83eb-bc27-438b-9025-d6071df40304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a bar plot showing the distribution of sentiments\n",
    "bar_plot(\n",
    "    reviews_df['Sentiment'].value_counts().sort_index().index, \n",
    "    reviews_df['Sentiment'].value_counts().sort_index().values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57499fdd-e23f-4eca-b178-3cbbe40a5e49",
   "metadata": {},
   "source": [
    "<h2>5. Generating Embeddings with BERTimbau</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7a21b8-5c9f-4e31-8793-683d13b9d002",
   "metadata": {},
   "source": [
    "<h3>5.1 Load BERTimbau Model and Tokenizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba9d84-fad4-4e64-9579-dc03cc234a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6736d00-b033-44dc-a6db-d7477b99d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pre-trained model name\n",
    "MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\"\n",
    "\n",
    "# Load the tokenizer for the selected BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load the BERT model and move it to the appropriate device (CPU/GPU)\n",
    "model = BertModel.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d1b01c-c4d0-445e-bb41-99a5a65d502b",
   "metadata": {},
   "source": [
    "<h3>5.2 Generating Text Embeddings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533fa681-c6be-4537-b95a-16416c43e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(reviews, tokenizer, model, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generates BERT embeddings for a list of reviews using the [CLS] token.\n",
    "\n",
    "    Args:\n",
    "        reviews (list): List of review texts.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer from the BERTimbau model.\n",
    "        model (transformers.PreTrainedModel): BERTimbau model for generating embeddings.\n",
    "        device (str or torch.device): Device to run the model on (e.g., \"cpu\" or \"cuda\").\n",
    "        batch_size (int, optional): Number of reviews processed per batch. Default is 32.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Matrix containing the embeddings for all reviews.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "\n",
    "    # Process reviews in batches\n",
    "    for i in tqdm(range(0, len(reviews), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch = reviews[i : i + batch_size]\n",
    "\n",
    "        # Tokenize the batch with padding and truncation\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract the [CLS] token embeddings (first token of each sequence)\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "    # Stack all batch embeddings into a single matrix\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0224d5-11f8-4609-9f20-8bfe343255e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for positive and negative keywords\n",
    "positive_embeddings = compute_embeddings(positive_reviews, tokenizer, model, device)\n",
    "negative_embeddings = compute_embeddings(negative_reviews, tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce21767-1e99-4568-8a1c-711ce6022ac7",
   "metadata": {},
   "source": [
    "<h2>6. Clustering Reviews with HDBSCAN</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15380b6-3618-4a50-a14d-a8a580605d69",
   "metadata": {},
   "source": [
    "<h3>6.1 Standard Scaling of Embeddings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65fcd8-1f47-4248-85fc-47cb90eaf565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_embeddings(embeddings):\n",
    "    \"\"\"\n",
    "    Applies standard scaling (zero mean, unit variance) to the embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): Input embeddings.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Scaled embeddings.\n",
    "    \"\"\"\n",
    "    # Initialize a StandardScaler, which scales features to have zero mean and unit variance\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler to the embeddings and transform them\n",
    "    return scaler.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cf49b-2726-41ee-85c2-24e7374d44d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Standard Scaler to the BERT embeddings for positive and negative reviews\n",
    "scaled_positive_embeddings = scale_embeddings(positive_embeddings)\n",
    "scaled_negative_embeddings = scale_embeddings(negative_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b245fd9-9812-4223-96d2-bcba38d98bc9",
   "metadata": {},
   "source": [
    "<h3>6.2 Hyperparameter Optimization of UMAP and HDBSCAN Using Optuna</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473d787-8908-4419-bbbc-153f4b7df3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, param_ranges):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization using UMAP and HDBSCAN.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): An Optuna trial object to suggest hyperparameters.\n",
    "        X (array-like): Input data (embeddings) for dimensionality reduction and clustering.\n",
    "        param_ranges (dict): Dictionary containing ranges for hyperparameter tuning. Expected keys:\n",
    "            - \"n_neighbors\" (tuple): Min and max values for UMAP's n_neighbors.\n",
    "            - \"min_dist\" (tuple): Min and max values for UMAP's min_dist.\n",
    "            - \"n_components\" (list): List of UMAP output dimensions to choose from.\n",
    "            - \"min_cluster_size\" (tuple): Min and max values for HDBSCAN's min_cluster_size.\n",
    "            - \"min_samples\" (tuple): Min and max values for HDBSCAN's min_samples.\n",
    "            - \"n_clusters\" (tuple): Acceptable range for the number of clusters.\n",
    "\n",
    "    Returns:\n",
    "        float: The silhouette score of the clustering result (higher is better).\n",
    "               Returns -1.0 if the configuration is penalized due to undesirable results.\n",
    "    \"\"\"\n",
    "    # Define hyperparameters using the provided parameter ranges\n",
    "    n_neighbors = trial.suggest_int(\"n_neighbors\", *param_ranges[\"n_neighbors\"])\n",
    "    min_dist = trial.suggest_float(\"min_dist\", *param_ranges[\"min_dist\"])\n",
    "    n_components = trial.suggest_categorical(\"n_components\", param_ranges[\"n_components\"])\n",
    "    min_cluster_size = trial.suggest_int(\"min_cluster_size\", *param_ranges[\"min_cluster_size\"])\n",
    "    min_samples = trial.suggest_int(\"min_samples\", *param_ranges[\"min_samples\"])\n",
    "\n",
    "    # Apply UMAP for dimensionality reduction (using cosine distance)\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric='cosine',\n",
    "        n_components=n_components,\n",
    "        random_state=42\n",
    "    )\n",
    "    X_umap = umap_model.fit_transform(X)\n",
    "\n",
    "    # Apply HDBSCAN clustering (using Euclidean distance)\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "    labels = clusterer.fit_predict(X_umap)\n",
    "\n",
    "    # Compute the number of clusters and the outlier ratio\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_outliers = np.sum(labels == -1)\n",
    "    outlier_ratio = n_outliers / len(labels)\n",
    "\n",
    "    # Retrieve cluster count limits from param_ranges\n",
    "    min_clusters, max_clusters = param_ranges[\"n_clusters\"]\n",
    "\n",
    "    # Penalize configurations with too few/many clusters or too many outliers\n",
    "    if n_clusters < min_clusters or n_clusters > max_clusters or outlier_ratio > 0.3:\n",
    "        return -1.0\n",
    "\n",
    "    # Compute silhouette score for clustering quality assessment\n",
    "    score = silhouette_score(X_umap, labels)\n",
    "\n",
    "    # Penalize negative silhouette scores\n",
    "    if score < 0:\n",
    "        return -1.0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e98ee-e6e7-411e-bfcc-321fe487fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_optuna_history(study):\n",
    "    \"\"\"\n",
    "    Plots the optimization history of an Optuna study.\n",
    "\n",
    "    Args:\n",
    "        study (optuna.study.Study): The Optuna study to visualize.\n",
    "    \"\"\"\n",
    "    # Generate the optimization history plot using Optuna's built-in function\n",
    "    fig = plot_optimization_history(study)\n",
    "\n",
    "    # Display the interactive plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed9210-d5a9-4e4e-a536-ea9dec09f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_umap_hdbscan_with_optuna(X, param_ranges, study_name, n_trials=100, seed=42):\n",
    "    \"\"\"\n",
    "    Optimizes UMAP and HDBSCAN hyperparameters using Optuna.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): Input data (embeddings) to be used for dimensionality reduction and clustering.\n",
    "        param_ranges (dict): Dictionary containing the hyperparameter ranges for tuning. Must include:\n",
    "            - \"n_neighbors\": Tuple[int, int] - Range for UMAP's number of neighbors.\n",
    "            - \"min_dist\": Tuple[float, float] - Range for UMAP's minimum distance.\n",
    "            - \"n_components\": List[int] - Options for UMAP's output dimensions.\n",
    "            - \"min_cluster_size\": Tuple[int, int] - Range for HDBSCAN's minimum cluster size.\n",
    "            - \"min_samples\": Tuple[int, int] - Range for HDBSCAN's minimum samples.\n",
    "            - \"n_clusters\": Tuple[int, int] - Acceptable range for the number of clusters.\n",
    "        study_name (str): Name for the Optuna study.\n",
    "        n_trials (int, optional): Number of trials to run in the optimization. Defaults to 100.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        optuna.study.Study: The Optuna study object containing optimization results.\n",
    "    \"\"\"\n",
    "    # Define sampler with fixed seed for reproducibility\n",
    "    sampler = TPESampler(seed=seed)\n",
    "\n",
    "    # Create an Optuna study (or load existing one) to maximize the silhouette score\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        direction=\"maximize\",\n",
    "        sampler=sampler,\n",
    "        storage=\"sqlite:///../logs/optuna/optuna_topic_modeling.db\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    # Run the optimization trials\n",
    "    study.optimize(lambda trial: objective(trial, X, param_ranges), n_trials=n_trials)\n",
    "\n",
    "    # Display best hyperparameters and score\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(study.best_params)\n",
    "    print(\"Best silhouette score:\", study.best_value)\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cde388-5914-48b3-8fc0-b7dfa9bb2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter ranges for clustering on positive sentiment reviews\n",
    "positive_param_ranges = {\n",
    "    \"n_neighbors\": (5, 10),\n",
    "    \"min_dist\": (0.0, 0.01),\n",
    "    \"n_components\": [3, 5, 10],\n",
    "    \"min_cluster_size\": (50, 150),\n",
    "    \"min_samples\": (10, 40),\n",
    "    \"n_clusters\": (10, 30)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153f771-f24a-4f98-b88d-fabc78500687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Optuna optimization for clustering on positive sentiment embeddings\n",
    "positive_optimization_results = optimize_umap_hdbscan_with_optuna(scaled_positive_embeddings, \n",
    "                                                                  positive_param_ranges,\n",
    "                                                                  study_name=\"topic_modeling_positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c4d20-89de-417c-a145-c00eb7d57f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimization history of the positive embeddings results\n",
    "plot_optuna_history(positive_optimization_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f8d47-3ce9-4462-b05b-49f6d48e2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter ranges for clustering on negative sentiment reviews\n",
    "negative_param_ranges = {\n",
    "    \"n_neighbors\": (5, 10),\n",
    "    \"min_dist\": (0.0, 0.01),\n",
    "    \"n_components\": [3, 5, 10],\n",
    "    \"min_cluster_size\": (10, 60),\n",
    "    \"min_samples\": (5, 25),\n",
    "    \"n_clusters\": (5, 15)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edbe7d2-85c0-40c4-8016-d1c2d258430d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Optuna optimization for clustering on positive sentiment embeddings\n",
    "negative_optimization_results = optimize_umap_hdbscan_with_optuna(scaled_negative_embeddings, \n",
    "                                                                  negative_param_ranges, \n",
    "                                                                  study_name=\"topic_modeling_negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4433d-2559-4585-a9b9-8adbf5185323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimization history of the negative embeddings results\n",
    "plot_optuna_history(negative_optimization_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bbea6f-a78e-4a68-8c09-6ae5a0866001",
   "metadata": {},
   "source": [
    "<h3>6.3 Dimensionality Reduction with UMAP</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd42b5d-b93f-4ac5-ba03-21113b0c7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_with_umap(embeddings, n_components=2, n_neighbors=15, min_dist=0.0, random_state=42):\n",
    "    \"\"\"\n",
    "    Reduces high-dimensional embeddings using UMAP.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): Input high-dimensional vectors, typically scaled.\n",
    "        n_components (int): Number of dimensions to reduce the embeddings to (e.g., 2 for visualization).\n",
    "        n_neighbors (int): Number of neighboring points used in the local approximation of the manifold.\n",
    "        min_dist (float): Minimum distance allowed between points in the low-dimensional space. \n",
    "                          Smaller values preserve more local structure.\n",
    "        random_state (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Embeddings reduced to the specified number of dimensions.\n",
    "    \"\"\"\n",
    "    # Initialize UMAP with specified parameters\n",
    "    umap_model = UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric='cosine',\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Fit UMAP on the embeddings and return the reduced output\n",
    "    reduced = umap_model.fit_transform(embeddings)\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7e3e27-e3d8-4e68-bceb-dee29a1b8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dimensionality of the positive and negative embeddings to 2D using UMAP\n",
    "positive_umap_embeddings = reduce_with_umap(scaled_positive_embeddings, n_components=5, n_neighbors=5, min_dist=0.000685789716338362)\n",
    "negative_umap_embeddings = reduce_with_umap(scaled_negative_embeddings, n_components=5, n_neighbors=10, min_dist=0.005694198925849235)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64aad41-bf34-4e25-a0a7-1b2e133118f1",
   "metadata": {},
   "source": [
    "<h3>6.4 Evaluating HDBSCAN Performance Across min_cluster_size Values</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafacdd-270b-496b-b4d6-d598108d690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hdbscan(embeddings, min_cluster_range=range(5, 30), min_samples=21):\n",
    "    \"\"\"\n",
    "    Evaluates HDBSCAN clustering across a range of minimum cluster sizes.\n",
    "\n",
    "    For each `min_cluster_size` value in the provided range, the function computes:\n",
    "    - The number of clusters identified (excluding noise points).\n",
    "    - The Silhouette Score of the clustering (excluding noise).\n",
    "    - The percentage of points classified as noise (label -1).\n",
    "\n",
    "    Args:\n",
    "        embeddings (array-like): Input array of vector embeddings (e.g., UMAP-reduced embeddings).\n",
    "        min_cluster_range (Iterable[int], optional): A sequence of `min_cluster_size` values to test. \n",
    "            Defaults to range(5, 30).\n",
    "        min_samples (int, optional): The number of samples in a neighborhood for a point to be considered\n",
    "            a core point. Default is 21, consistent with the main project configuration.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the following keys:\n",
    "            - 'min_cluster_size' (List[int]): Values of `min_cluster_size` evaluated.\n",
    "            - 'num_clusters' (List[int]): Number of clusters found for each setting.\n",
    "            - 'silhouette_scores' (List[float]): Silhouette Score (excluding noise) for each setting.\n",
    "            - 'outlier_percentages' (List[float]): Percentage of points identified as noise for each setting.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store evaluation metrics\n",
    "    num_clusters = []\n",
    "    silhouette_scores = []\n",
    "    outlier_percentages = []\n",
    "\n",
    "    # Iterate over each min_cluster_size value in the specified range\n",
    "    for min_size in min_cluster_range:\n",
    "        # Initialize the HDBSCAN clusterer with the current min_cluster_size\n",
    "        clusterer = HDBSCAN(min_cluster_size=min_size, \n",
    "                            min_samples=min_samples, \n",
    "                            metric='euclidean',\n",
    "                            cluster_selection_method='eom')\n",
    "\n",
    "        # Fit the model and predict cluster labels\n",
    "        cluster_labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "        # Calculate the number of clusters found (excluding noise)\n",
    "        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "        num_clusters.append(n_clusters)\n",
    "\n",
    "        # Create a mask for non-noise points (i.e., points not labeled as -1)\n",
    "        mask = cluster_labels != -1\n",
    "        if mask.sum() > 1:\n",
    "            # Compute the Silhouette Score for non-noise points\n",
    "            score = silhouette_score(embeddings[mask], cluster_labels[mask])\n",
    "        else:\n",
    "            # Assign NaN if there are too few non-noise points to compute the score\n",
    "            score = np.nan\n",
    "        silhouette_scores.append(score)\n",
    "\n",
    "        # Calculate the percentage of points classified as noise\n",
    "        outlier_pct = np.sum(cluster_labels == -1) / len(cluster_labels) * 100\n",
    "        outlier_percentages.append(outlier_pct)\n",
    "\n",
    "    # Compile the results into a dictionary\n",
    "    results = {\n",
    "        'min_cluster_size': list(min_cluster_range),\n",
    "        'num_clusters': num_clusters,\n",
    "        'silhouette_scores': silhouette_scores,\n",
    "        'outlier_percentages': outlier_percentages\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef8bf0-1e8d-466d-9e72-5812121646f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hdbscan_results(results, category):\n",
    "    \"\"\"\n",
    "    Plots HDBSCAN evaluation metrics across different min_cluster_size values.\n",
    "\n",
    "    This function generates a figure with three subplots:\n",
    "    1. Number of clusters found vs. min_cluster_size.\n",
    "    2. Silhouette score vs. min_cluster_size.\n",
    "    3. Outlier percentage vs. min_cluster_size.\n",
    "\n",
    "    The figure is saved to a path depending on the label provided.\n",
    "\n",
    "    Args:\n",
    "        results (dict): A dictionary containing the following keys:\n",
    "            - 'min_cluster_size' (List[int]): Values of min_cluster_size tested.\n",
    "            - 'num_clusters' (List[int]): Number of clusters found for each setting.\n",
    "            - 'silhouette_scores' (List[float]): Silhouette score for each configuration.\n",
    "            - 'outlier_percentages' (List[float]): Percentage of data points classified as outliers.\n",
    "        category (str): Category label (e.g., 'positive' or 'negative') to customize output path.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays and saves the matplotlib figure.\n",
    "    \"\"\"\n",
    "    # Extract the list of min_cluster_size values from the results dictionary\n",
    "    min_cluster_sizes = results['min_cluster_size']\n",
    "\n",
    "    # Create a figure with 3 subplots arranged horizontally\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Set a global title for the figure\n",
    "    fig.suptitle(f\"HDBSCAN Clustering Performance for {category.capitalize()} Reviews\",\n",
    "                 fontfamily='Arial Rounded MT Bold', fontsize=18)\n",
    "\n",
    "    # ---- Plot 1: Number of Clusters vs. min_cluster_size ----\n",
    "    axs[0].plot(min_cluster_sizes, results['num_clusters'],\n",
    "                marker='o', linestyle='-', color='#0d47a1')\n",
    "    axs[0].set_title(\"Number of Clusters\",\n",
    "                     fontfamily='Arial Rounded MT Bold', fontsize=14)\n",
    "    axs[0].set_xlabel(\"min_cluster_size\", fontsize=12)\n",
    "    axs[0].set_ylabel(\"Number of Clusters\", fontsize=12)\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # ---- Plot 2: Silhouette Score vs. min_cluster_size ----\n",
    "    axs[1].plot(min_cluster_sizes, results['silhouette_scores'],\n",
    "                marker='o', linestyle='-', color='#2E7D32')\n",
    "    axs[1].set_title(\"Silhouette Score\",\n",
    "                     fontfamily='Arial Rounded MT Bold', fontsize=14)\n",
    "    axs[1].set_xlabel(\"min_cluster_size\", fontsize=12)\n",
    "    axs[1].set_ylabel(\"Silhouette Score\", fontsize=12)\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # ---- Plot 3: Outlier Percentage vs. min_cluster_size ----\n",
    "    axs[2].plot(min_cluster_sizes, results['outlier_percentages'],\n",
    "                marker='o', linestyle='-', color='#c62828')\n",
    "    axs[2].set_title(\"Outlier Percentage\",\n",
    "                     fontfamily='Arial Rounded MT Bold', fontsize=14)\n",
    "    axs[2].set_xlabel(\"min_cluster_size\", fontsize=12)\n",
    "    axs[2].set_ylabel(\"Outlier Percentage (%)\", fontsize=12)\n",
    "    axs[2].grid(True)\n",
    "\n",
    "    # Automatically adjust layout to avoid label overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the complete figure\n",
    "    plt.show()\n",
    "\n",
    "    # Save the figure with appropriate path based on label\n",
    "    output_path = os.path.join(\"..\", \"results\", \"figures\", \"topic_modeling\", f\"{category}_hdbscan_performance.png\")\n",
    "    save_plot(fig, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3959c7b-61fc-4cb5-90b0-f600fc37f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate HDBSCAN clustering performance on the positive UMAP embeddings\n",
    "positive_hdbscan_results = evaluate_hdbscan(positive_umap_embeddings, range(50, 90), 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51582d01-4167-4085-908b-b6e130cfa2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the HDBSCAN clustering evaluation results for the positive embeddings\n",
    "plot_hdbscan_results(positive_hdbscan_results, category=\"positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ac5e59-935d-4e42-9648-1c9213a7d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate HDBSCAN clustering performance on the negative UMAP embeddings\n",
    "negative_hdbscan_results = evaluate_hdbscan(negative_umap_embeddings, range(5, 30), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5ba9a-ecde-4dcb-9f7c-6586dfbe3797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the HDBSCAN clustering evaluation results for the negative embeddings\n",
    "plot_hdbscan_results(negative_hdbscan_results, category=\"negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad3a06d-ae31-4089-95a1-02c57da29e89",
   "metadata": {},
   "source": [
    "<h3>6.5 Application of the HDBSCAN Algorithm to 2D Embeddings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e401e-004a-4284-a7f7-d11289889b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_with_hdbscan(embeddings_2d, min_cluster_size=5, min_samples=10):\n",
    "    \"\"\"\n",
    "    Applies the HDBSCAN clustering algorithm to 2D embeddings and returns cluster labels along with membership probabilities.\n",
    "\n",
    "    Args:\n",
    "        embeddings_2d (array-like): Data points in 2D space to be clustered.\n",
    "        min_cluster_size (int, optional): Minimum number of samples in a cluster. Defaults to 5.\n",
    "        min_samples (int, optional): Controls how conservative the clustering is. Higher values lead to more noise points. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            labels (ndarray): Cluster labels assigned to each point. Points labeled as -1 are considered noise.\n",
    "            probabilities_dict (dict): Dictionary with key `'probabilities'` containing the membership strength (confidence) for each point.\n",
    "    \"\"\"\n",
    "    # Convert input embeddings to NumPy array if not already\n",
    "    embeddings_2d = np.asarray(embeddings_2d)\n",
    "\n",
    "    # Initialize the HDBSCAN clusterer with specified hyperparameters\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "\n",
    "    # Fit the clusterer and get predicted cluster labels\n",
    "    labels = clusterer.fit_predict(embeddings_2d)\n",
    "\n",
    "    # Retrieve the soft clustering probabilities (membership strength per point)\n",
    "    probabilities = clusterer.probabilities_\n",
    "\n",
    "    # Store probabilities in a dictionary for consistent output format\n",
    "    probabilities_dict = {'probabilities': probabilities}\n",
    "\n",
    "    # Return both the labels and the probability dictionary\n",
    "    return labels, probabilities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e429d-bc0a-4976-9e72-fdee6d07d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster positive UMAP embeddings using HDBSCAN\n",
    "positive_labels, _ = cluster_with_hdbscan(positive_umap_embeddings, min_cluster_size=70, min_samples=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35170d5e-fb58-490b-8548-c3c930dcca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster negative UMAP embeddings using HDBSCAN\n",
    "negative_labels, _ = cluster_with_hdbscan(negative_umap_embeddings, min_cluster_size=25, min_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e234ba2-a7e2-4c5d-86a3-380e502b884a",
   "metadata": {},
   "source": [
    "<h3>6.6 Visualizing Clusters with UMAP and HDBSCAN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688d94e-eeac-4f2d-862d-ef40520b497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_clusters(umap_embeddings, labels, category, title='UMAP + HDBSCAN Clusters'):\n",
    "    \"\"\"\n",
    "    Plot UMAP embeddings colored by cluster labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    umap_embeddings : ndarray of shape (n_samples, 2)\n",
    "        The 2D UMAP-reduced embeddings.\n",
    "\n",
    "    labels : array-like of shape (n_samples,)\n",
    "        Cluster labels assigned to each data point (e.g., from HDBSCAN).\n",
    "        The label -1 typically represents noise or unclustered points.\n",
    "\n",
    "    title : str, optional (default='UMAP + HDBSCAN Clusters')\n",
    "        Title to display on the plot.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Displays the plot of UMAP embeddings with cluster coloring.\n",
    "    \"\"\"\n",
    "    # Create a figure and axis using the object-oriented interface\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Get the unique cluster labels (including noise)\n",
    "    unique_labels = set(labels)\n",
    "\n",
    "    # Generate a color palette with as many colors as unique labels\n",
    "    colors = sns.color_palette('hsv', len(unique_labels))\n",
    "\n",
    "    # Plot each cluster separately\n",
    "    for label, color in zip(unique_labels, colors):\n",
    "        # Create a mask for points belonging to the current cluster\n",
    "        mask = labels == label\n",
    "        # Assign a label name: \"Cluster X\" or \"Noise\" if label == -1\n",
    "        label_name = f\"Cluster {label}\" if label != -1 else \"Noise\"\n",
    "        # Scatter plot for the current cluster\n",
    "        ax.scatter(umap_embeddings[mask, 0], umap_embeddings[mask, 1],\n",
    "                   c=[color], label=label_name, alpha=0.7, s=40)\n",
    "\n",
    "    # Set axis labels\n",
    "    ax.set_xlabel(\"UMAP Dimension 1\", fontsize=12)\n",
    "    ax.set_ylabel(\"UMAP Dimension 2\", fontsize=12)\n",
    "\n",
    "    # Set the plot title\n",
    "    ax.set_title(title, fontfamily='Arial Rounded MT Bold', fontsize=16, pad=15)\n",
    "\n",
    "    # Configure legend\n",
    "    ax.legend(title='Clusters', loc='best', frameon=True, fontsize=10, title_fontsize=11)\n",
    "\n",
    "    # Configure grid\n",
    "    ax.grid(visible=True, linestyle='--', color='gray', alpha=0.7)\n",
    "\n",
    "    # Adjust layout and display the plot\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the plot to a standardized output path using the category\n",
    "    output_path = os.path.join(\"..\", \"results\", \"figures\", \"topic_modeling\", f\"{category}_umap_clusters.png\")\n",
    "    save_plot(fig, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca4c7d-99aa-4049-a8c5-a25af91fb57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot UMAP projection of positive review embeddings, colored by their assigned cluster labels\n",
    "plot_umap_clusters(positive_umap_embeddings, positive_labels, category=\"positive\", title='Clusters of Positive Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae392034-0720-4542-b1c9-eadcd8bb155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot UMAP projection of negative review embeddings, colored by their assigned cluster labels\n",
    "plot_umap_clusters(negative_umap_embeddings, negative_labels, category=\"negative\", title='Clusters of Negative Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ceb53-7051-45bf-97e9-0dad7b33d072",
   "metadata": {},
   "source": [
    "<h3>6.7 Group Reviews by Cluster (Excluding Noise)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2481ad-9fef-4898-ac86-7c82c98f07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Export results dictionary to a JSON file, ensuring JSON compatibility.\n",
    "\n",
    "    This function converts any non-native Python keys (e.g., numpy.int64, numpy.float64)\n",
    "    and values (e.g., numpy types, arrays) into standard Python types, and then\n",
    "    writes the result into a JSON file at the specified path.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary containing the results to be exported. Keys can be\n",
    "            cluster labels or other identifiers, and values can include lists or\n",
    "            nested structures.\n",
    "        file_path (str): Full path (including the file name) where the JSON file\n",
    "            will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If an object in the dictionary cannot be serialized to JSON.\n",
    "\n",
    "    Note:\n",
    "        Requires `numpy` for type checking and conversion.\n",
    "    \"\"\"\n",
    "    \n",
    "    def convert(o):\n",
    "        if isinstance(o, (np.integer, np.int64)):\n",
    "            return int(o)\n",
    "        elif isinstance(o, (np.floating, np.float32, np.float64)):\n",
    "            return float(o)\n",
    "        elif isinstance(o, np.ndarray):\n",
    "            return o.tolist()\n",
    "        else:\n",
    "            raise TypeError(f\"Object of type {type(o).__name__} is not JSON serializable\")\n",
    "\n",
    "    # Convert dictionary keys to native Python types if needed\n",
    "    converted_data = {\n",
    "        int(key) if isinstance(key, (np.integer, np.floating)) else key: value\n",
    "        for key, value in data.items()\n",
    "    }\n",
    "\n",
    "    # Write the data to JSON file with proper formatting\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(converted_data, f, ensure_ascii=False, indent=2, default=convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc726c3b-aeea-42ca-988f-a1f0c400e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts_by_cluster_no_noise(texts, labels, exclude_noise=True):\n",
    "    \"\"\"\n",
    "    Groups review texts by their corresponding cluster labels.\n",
    "    \n",
    "    When `exclude_noise=True`, all texts labeled as noise (-1) are ignored, so the result\n",
    "    includes only clean, clustered data.\n",
    "\n",
    "    Args:\n",
    "        texts (Iterable[str]): List of review texts.\n",
    "        labels (Iterable[int]): Cluster labels assigned to each text (e.g., from HDBSCAN).\n",
    "        exclude_noise (bool, optional): If True, excludes texts labeled as noise (-1). Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary where keys are cluster labels and values are lists of texts belonging to each cluster.\n",
    "\n",
    "    Example:\n",
    "        >>> grouped = group_texts_by_cluster(my_reviews, my_labels)\n",
    "        >>> grouped[0]  # All texts from cluster 0\n",
    "    \"\"\"\n",
    "    clustered_texts = defaultdict(list)\n",
    "\n",
    "    for text, label in zip(texts, labels):\n",
    "        # If exclude_noise is True, skip texts labeled as noise (-1)\n",
    "        if exclude_noise and label == -1:\n",
    "            continue\n",
    "        # Check if the text is a valid, non-empty string\n",
    "        if isinstance(text, str) and text.strip():\n",
    "            # Remove any leading/trailing whitespace and add the text to the corresponding cluster\n",
    "            clustered_texts[label].append(text.strip())\n",
    "\n",
    "    return dict(clustered_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc2847-b841-4f95-94d8-926c9749776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group positive reviews by cluster while excluding noise points\n",
    "positive_clustered_reviews = group_texts_by_cluster_no_noise(positive_reviews, positive_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b0f191-dd2b-4868-9e1f-5e0c4300d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group negative reviews by cluster while excluding noise points\n",
    "negative_clustered_reviews = group_texts_by_cluster_no_noise(negative_reviews, negative_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db188137-3684-471f-99e3-73786ed3193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the positive clustered reviews data to a JSON file in the specified path.\n",
    "export_results_json(\n",
    "    positive_clustered_reviews, \n",
    "    os.path.join(\"..\", \"results\", \"clustering\", \"positive_clustered_reviews.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b07f9-7232-4986-8652-e9c7d48ecc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the negative clustered reviews data to a JSON file in the specified path.\n",
    "export_results_json(\n",
    "    negative_clustered_reviews, \n",
    "    os.path.join(\"..\", \"results\", \"clustering\", \"negative_clustered_reviews.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71cff85-336a-480b-b836-a994239bf132",
   "metadata": {},
   "source": [
    "<h2>8. Topic Extraction and Clustering Using BERTimbau</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250c102-7426-49ed-b371-905067e3f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_batch(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Computes BERT embeddings for a batch of texts.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of input texts.\n",
    "        batch_size (int, optional): Number of texts to process at a time. Default is 32.\n",
    "\n",
    "    Returns:\n",
    "        list of torch.Tensor: List of sentence embeddings.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "        # Tokenize the batch with padding and truncation\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract hidden states from the last layer\n",
    "        embeddings = outputs.last_hidden_state  # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Expand attention mask to match the embeddings shape\n",
    "        attention_mask = inputs['attention_mask'].unsqueeze(-1).expand(embeddings.size())\n",
    "\n",
    "        # Apply attention mask to embeddings\n",
    "        masked_embeddings = embeddings * attention_mask\n",
    "\n",
    "        # Sum embeddings across the sequence length dimension\n",
    "        summed = masked_embeddings.sum(dim=1)\n",
    "\n",
    "        # Count the number of valid (non-padded) tokens\n",
    "        counts = attention_mask.sum(dim=1)\n",
    "\n",
    "        # Compute the mean embedding per sentence\n",
    "        mean_embeddings = (summed / counts).cpu()\n",
    "\n",
    "        # Store the computed embeddings\n",
    "        all_embeddings.extend(mean_embeddings)\n",
    "\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34c5320-50f9-4371-bb38-7a47197a9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, ngram_range=(1, 3)):\n",
    "    \"\"\"\n",
    "    Generates n-grams from a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        ngram_range (tuple, optional): Range of n-grams to generate (min_n, max_n). Default is (1, 3).\n",
    "\n",
    "    Returns:\n",
    "        list of str: Unique n-grams extracted from the text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    ngrams = []\n",
    "\n",
    "    # Generate n-grams for each value of n in the given range\n",
    "    for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "        for i in range(len(words) - n + 1):\n",
    "            ngram = ' '.join(words[i:i + n])\n",
    "            ngrams.append(ngram)\n",
    "\n",
    "    return list(set(ngrams))  # Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a478591b-a8d3-4386-b9c9-5e144fb5891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text, max_tokens=512):\n",
    "    \"\"\"\n",
    "    Truncates a text to a maximum number of tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        max_tokens (int, optional): Maximum number of tokens allowed. Default is 512.\n",
    "\n",
    "    Returns:\n",
    "        str: Truncated text converted back to a string.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Return the original text if it is within the token limit\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "\n",
    "    # Truncate tokens to the maximum limit\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    return tokenizer.convert_tokens_to_string(truncated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600680ef-8198-4db4-b11a-c6b6921c154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_bertimbau(text, top_n=5, ngram_range=(1, 3)):\n",
    "    \"\"\"\n",
    "    Extracts the most relevant keywords from a given text using BERTimbau embeddings.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        top_n (int, optional): Number of top keywords to return. Default is 5.\n",
    "        ngram_range (tuple, optional): Range of n-grams to consider. Default is (1, 3).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples containing the top keywords and their similarity scores.\n",
    "    \"\"\"\n",
    "    # Generate the document embedding\n",
    "    doc_embedding = get_embedding_batch([text])[0]  # get_embedding_batch returns a list of embeddings\n",
    "\n",
    "    # Generate candidate n-grams from the text\n",
    "    candidates = generate_ngrams(text, ngram_range=ngram_range)\n",
    "\n",
    "    # Compute embeddings for the candidate n-grams\n",
    "    candidate_embeddings = get_embedding_batch(candidates)  # Returns embeddings for all candidates\n",
    "\n",
    "    # Compute cosine similarity between the document embedding and candidate embeddings\n",
    "    similarities = cosine_similarity([doc_embedding], candidate_embeddings).flatten()\n",
    "\n",
    "    # Rank candidates by similarity score in descending order\n",
    "    ranked = sorted(zip(candidates, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7959b1d-4982-473b-9389-264fc9c91d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_from_clusters_bertimbau(clustered_texts, top_n=5, ngram_range=(2, 3)):\n",
    "    \"\"\"\n",
    "    Extracts the most relevant keywords for each text cluster using BERTimbau embeddings.\n",
    "\n",
    "    Args:\n",
    "        clustered_texts (dict): Dictionary where keys are cluster IDs and values are lists of texts.\n",
    "        top_n (int, optional): Number of top keywords to extract for each cluster. Default is 5.\n",
    "        ngram_range (tuple, optional): Range of n-grams to consider. Default is (2, 3).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each cluster ID to a list of extracted keywords and their scores.\n",
    "    \"\"\"\n",
    "    cluster_keywords = {}\n",
    "\n",
    "    # Iterate over each cluster\n",
    "    for cluster_id, texts in tqdm(clustered_texts.items(), desc=\"Extracting cluster keywords\"):\n",
    "        combined_text = ' '.join(texts)  # Combine all texts within the cluster\n",
    "        keywords = extract_keywords_bertimbau(combined_text, top_n=top_n, ngram_range=ngram_range)\n",
    "        cluster_keywords[cluster_id] = keywords  # Store keywords for each cluster\n",
    "\n",
    "    return cluster_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d53b9d-cccb-4174-9a92-299bdf4d342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_dataframe(topic_dict):\n",
    "    \"\"\"\n",
    "    Converts a dictionary of topic clusters into a structured pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        topic_dict (dict): A dictionary where keys are cluster labels (integers) and \n",
    "                           values are lists of tuples containing topic phrases and their scores.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns 'Cluster', 'Topic', and 'Score'.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for cluster, topics in topic_dict.items():\n",
    "        for topic, score in topics:\n",
    "            data.append({'Cluster': cluster, 'Topic': topic, 'Score': score})\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e15fce3-6351-4f4f-bca1-83c039b17958",
   "metadata": {},
   "source": [
    "<h3>8.1 Keyword Extraction and Clustering for Positive Reviews</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016aba2-455a-489e-a6b5-d56f803e1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts top 3 keywords for each cluster in the positive reviews\n",
    "positive_cluster_topics = extract_keywords_from_clusters_bertimbau(  \n",
    "    clustered_texts=positive_clustered_reviews,  \n",
    "    top_n=5,  \n",
    "    ngram_range=(2, 3)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477bb42f-6846-4552-afba-dac4d8283726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the extracted topics from positive clusters to a JSON file at the specified path.\n",
    "export_results_json(\n",
    "    positive_cluster_topics, \n",
    "    os.path.join(\"..\", \"results\", \"clustering\", \"positive_cluster_topics.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f72c0-e627-40ba-adcc-e5eb80079689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the positive cluster topics into a DataFrame\n",
    "positive_topics_df = convert_dict_to_dataframe(positive_cluster_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f273f06-3bc5-4d48-96d6-9f72f0010b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_topics_df.sort_values(by='Cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b0dccb-48f5-4452-8b33-90932ecdfbb7",
   "metadata": {},
   "source": [
    "<h3>8.2 Keyword Extraction and Clustering for Negative Reviews</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c946aa-8404-4f63-853a-ee4aa70d55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts top 3 keywords for each cluster in the negative reviews\n",
    "negative_cluster_topics = extract_keywords_from_clusters_bertimbau(  \n",
    "    clustered_texts=negative_clustered_reviews,  \n",
    "    top_n=5,  \n",
    "    ngram_range=(2, 3)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2080f4e-6c43-45a7-8d9d-4071011acef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the extracted topics from negative clusters to a JSON file at the specified path.\n",
    "export_results_json(\n",
    "    negative_cluster_topics, \n",
    "    os.path.join(\"..\", \"results\", \"clustering\", \"negative_cluster_topics.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5911c416-2291-4cdc-8841-dad9e9b35cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the negative cluster topics into a DataFrame\n",
    "negative_topics_df = convert_dict_to_dataframe(negative_cluster_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73e7c4-aec2-4234-8335-587d315472ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_topics_df.sort_values(by='Cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2ae74-6978-42eb-a57a-5e3e74feaa10",
   "metadata": {},
   "source": [
    "<h2>9. Summarize Topic Clusters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca155e97-01ca-4e2c-bf50-5116c87a662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cluster_labels(cluster_dict, top_n_keywords=3):\n",
    "    \"\"\"\n",
    "    Generates label suggestions for each cluster based on its most representative phrases\n",
    "    using KeyBERT to extract keywords.\n",
    "\n",
    "    Args:\n",
    "        cluster_dict (dict): Dictionary formatted as {cluster_id: [(phrase, score), ...]}.\n",
    "            Each value is a list of tuples containing a representative phrase and its importance score.\n",
    "        top_n_keywords (int): Number of keywords to extract and use in the suggested label.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with cluster_id as keys and suggested labels as values.\n",
    "    \"\"\"\n",
    "    # Initialize the KeyBERT model using a lightweight sentence transformer\n",
    "    kw_model = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    cluster_labels = {}\n",
    "\n",
    "    # Iterate through each cluster and generate a label\n",
    "    for cluster_id, phrases_scores in cluster_dict.items():\n",
    "        # Select the top-N most representative phrases for the current cluster\n",
    "        top_phrases = [phrase for phrase, _ in phrases_scores[:5]]\n",
    "\n",
    "        # Join the phrases into a single text block\n",
    "        joined_text = \" \".join(top_phrases)\n",
    "\n",
    "        # Extract keywords from the combined text\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            joined_text,\n",
    "            keyphrase_ngram_range=(2, 3),\n",
    "            top_n=top_n_keywords\n",
    "        )\n",
    "\n",
    "        # Combine the keywords into a comma-separated label string\n",
    "        label = \", \".join([kw for kw, _ in keywords])\n",
    "\n",
    "        # Assign the label to the current cluster\n",
    "        cluster_labels[cluster_id] = label\n",
    "\n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6a865-9915-4388-8d5e-23e323e7e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_summary_df(keybert_topics: dict, manual_summaries: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a DataFrame combining KeyBERT topics (already processed) and manual summaries by cluster,\n",
    "    and sorts the result by Cluster ID.\n",
    "\n",
    "    Args:\n",
    "        keybert_topics (dict): A dictionary where each key is a cluster ID and \n",
    "                               each value is a string or list of keywords already processed.\n",
    "        manual_summaries (dict): A dictionary with manual summaries for each cluster ID.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'Cluster ID', 'KeyBERT Topics', and 'Manual Summary',\n",
    "                      sorted by Cluster ID.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for cluster_id, keywords in keybert_topics.items():\n",
    "        if isinstance(keywords, list):\n",
    "            top_keywords = \", \".join(keywords)\n",
    "        else:\n",
    "            top_keywords = keywords  # assume it's already a string\n",
    "\n",
    "        summary = manual_summaries.get(cluster_id, \"No summary\")\n",
    "        data.append({\n",
    "            \"Cluster ID\": cluster_id,\n",
    "            \"KeyBERT Topics\": top_keywords,\n",
    "            \"Manual Summary\": summary\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data).sort_values(by=\"Cluster ID\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89d125-4c34-442c-9f07-d29abcd24f0b",
   "metadata": {},
   "source": [
    "<h3>9.1 Topic Summarization of Positive Clusters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d7599a-46c9-4cab-b14c-09978680d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate representative labels for each positive cluster using the top keywords\n",
    "positive_cluster_labels = generate_cluster_labels(positive_cluster_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55549012-abd0-412e-bf13-01dba00a0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print each cluster ID and its corresponding label in sorted order\n",
    "for cluster_id in sorted(positive_cluster_labels):\n",
    "    print(f\"Cluster {cluster_id}: {positive_cluster_labels[cluster_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681e42b3-c742-4061-8a65-9723d870eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually written summaries for each positive cluster\n",
    "positive_cluster_summaries = {\n",
    "    0: \"Excellent veterinarians with great care and attention.\",\n",
    "    1: \"Caring and friendly service; highly recommended.\",\n",
    "    2: \"Wonderful place with amazing staff and products.\",\n",
    "    3: \"Loved by pet owners; affectionate and reliable service.\",\n",
    "    4: \"Attentive professionals with a focus on detail and care.\",\n",
    "    5: \"High-quality service and good variety of pet food.\",\n",
    "    6: \"Very affectionate grooming and positive customer experience.\",\n",
    "    7: \"Outstanding service with warmth and appreciation.\",\n",
    "    8: \"Impeccable and affectionate work; strongly recommended.\",\n",
    "    9: \"Pets love the place; trusted and beloved service.\",\n",
    "    10: \"Impressive service with knowledgeable and kind staff.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1c9b4-c705-4839-9c85-e4f08aec805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame combining KeyBERT topics and manual summaries for positive clusters\n",
    "positive_topics_df = create_topic_summary_df(\n",
    "    keybert_topics=positive_cluster_labels,       # Dictionary with KeyBERT topics per cluster\n",
    "    manual_summaries=positive_cluster_summaries   # Dictionary with manual summaries per cluster\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b93b8a7-951e-42f4-8eaa-7c2c097b1131",
   "metadata": {},
   "source": [
    "<h3>9.2 Topic Summarization of Negative Clusters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a519f-d864-4a75-b1cb-333d643ae256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate representative labels for each negative cluster using the top keywords\n",
    "negative_cluster_labels = generate_cluster_labels(negative_cluster_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b767081-38fc-4683-b6dc-62b93ade31f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print each cluster ID and its corresponding label in sorted order\n",
    "for cluster_id in sorted(negative_cluster_labels):\n",
    "    print(f\"Cluster {cluster_id}: {negative_cluster_labels[cluster_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e308784-669a-45bc-889d-c181e3758623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually written summaries for each negative cluster\n",
    "negative_cluster_summaries = {\n",
    "    0: \"Poor service and customer dissatisfaction.\",\n",
    "    1: \"Negative experiences related to pet mistreatment.\",\n",
    "    2: \"Health issues and concerns with pet treatment.\",\n",
    "    3: \"Disappointment with care and service provided.\",\n",
    "    4: \"Sarcastic remarks and disbelief about diagnosis.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0855eb6f-1f42-4e31-961d-16bd788e7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame combining KeyBERT topics and manual summaries for negative clusters\n",
    "negative_topics_df = create_topic_summary_df(\n",
    "    keybert_topics=negative_cluster_labels,       # Dictionary with KeyBERT topics per cluster\n",
    "    manual_summaries=negative_cluster_summaries   # Dictionary with manual summaries per cluster\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afb6846-83b2-49f4-8d0f-23c914bcdc30",
   "metadata": {},
   "source": [
    "<h3>9.3 Save the Results to a CSV File</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167c622-7ebe-4e7d-b42d-cc2793da25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for saving the CSV file\n",
    "POSITIVE_TOPICS_PATH = os.path.join(\"..\", \"results\", \"clustering\", \"positive_cluster_topic_summaries.csv\")\n",
    "NEGATIVE_TOPICS_PATH = os.path.join(\"..\", \"results\", \"clustering\", \"negative_cluster_topic_summaries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264f3f7-e99f-49da-b860-d99d9f7e4162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a CSV file\n",
    "positive_topics_df.to_csv(POSITIVE_TOPICS_PATH, index=False, sep=\";\", encoding=\"utf-8\")\n",
    "negative_topics_df.to_csv(NEGATIVE_TOPICS_PATH, index=False, sep=\";\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166fbfad-fdab-4762-92b9-2ce1f977663d",
   "metadata": {},
   "source": [
    "<h2>10. Conclusion</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b9e719-d974-4448-9d47-47666e2968e2",
   "metadata": {},
   "source": [
    "This topic modeling approach enabled the identification of recurring themes in customer reviews related to pet services. The pipeline followed these main steps:\n",
    "\n",
    "- **BERTimbau embeddings** were used to generate semantic representations of each review in Brazilian Portuguese.  \n",
    "- **HDBSCAN** was applied to cluster the embeddings based on semantic density, without needing to predefine the number of topics.  \n",
    "- **BERTimbau** was used again to extract relevant keywords from each cluster.  \n",
    "- **KeyBERT** summarized and refined the keywords, producing interpretable labels for each topic.\n",
    "\n",
    "Negative reviews frequently focused on issues such as poor service, health concerns, and dissatisfaction with care. Positive reviews, on the other hand, highlighted attentive professionals, trust in pet handling, and overall satisfaction. These patterns were captured in concise summaries for each cluster, allowing for a clear distinction between commonly praised and criticized aspects of pet service businesses.\n",
    "\n",
    "The use of **HDBSCAN** was particularly effective in this context. One of the reasons for choosing HDBSCAN was the uncertainty about how many distinct topics would be present in the reviews. Unlike traditional clustering methods, HDBSCAN does not require the number of clusters to be specified in advance. Instead, it automatically detects the number of clusters based on the semantic density of the data, making it a more flexible and appropriate choice for this type of unstructured, complex text. \n",
    "\n",
    "To ensure well-formed clusters, all HDBSCAN and UMAP parameters were fine-tuned using **Optuna**, a hyperparameter optimization framework. During the parameter tuning process, the initial trials were performed using broad, random search intervals. This was necessary to explore the overall landscape of possible configurations and understand how different parameter ranges influenced the clustering quality. Once the initial search revealed promising regions in the parameter space, the search intervals were refined to focus on those narrower, more optimal ranges. This two-step tuning strategy helped improve the efficiency of the optimization process and resulted in more coherent and well-separated topic clusters, as measured by the **Silhouette Score**.\n",
    "\n",
    "While higher similarity scores often indicate that a topic phrase is semantically close to the cluster content, they don't necessarily guarantee that the topic is informative or diverse. On the **topic extraction using BERTimbau** step, this becomes particularly relevant: especially when using larger n-grams (e.g., bigrams or trigrams), scores tend to increase naturally due to richer context. However, this can lead to **redundant** or **overly generic phrases** that fail to summarize the core ideas effectively.\n",
    "\n",
    "It is important to acknowledge a key limitation of this project: due to restrictions imposed by the API, only a limited number of reviews could be retrieved per business. This constraint reduced the diversity and volume of textual data available for analysis. As a result, the topics extracted in this study may not fully capture the broader range of customer experiences and opinions within the pet services sector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f9a8c-49c1-4f51-98d8-ac600c29bd39",
   "metadata": {},
   "source": [
    "<h2>11. Next Steps</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b427f-d916-4656-a193-0b0f503225e8",
   "metadata": {},
   "source": [
    "Building upon the current results, several directions could enhance the robustness and relevance of the topic modeling process:\n",
    "\n",
    "- **Expand the dataset**  \n",
    "  Collect a broader and more diverse set of reviews, ideally overcoming current API limitations. This would improve topic coverage and provide a more representative view of customer experiences in the pet services sector.\n",
    "\n",
    "- **Apply the methodology to new domains or regions**  \n",
    "  Extend the approach to different geographical areas or business types within the pet care industry to compare topic distributions across contexts.\n",
    "\n",
    "- **Improve topic interpretability**  \n",
    "  Integrate filtering and ranking strategies that balance **semantic cohesion** and **diversity**, helping to surface more informative and actionable clusters.\n",
    "\n",
    "- **Refine cluster quality**  \n",
    "  Use alternative validation metrics and qualitative assessments to ensure that clusters remain meaningful, distinct, and robust.\n",
    "\n",
    "- **Enhance hyperparameter optimization**  \n",
    "  Adopt more advanced optimization strategies. In this project, a two-step approach was used: broad random exploration followed by narrowing the search space. Future iterations could include:\n",
    "  \n",
    "  - **Bayesian optimization with early pruning**  \n",
    "    Use Optuna's pruning mechanism to halt underperforming trials early based on intermediate metrics (e.g., partial Silhouette Scores), improving computational efficiency.\n",
    "\n",
    "  - **Multi-objective optimization**  \n",
    "    Optimize for multiple objectives simultaneously (e.g., cohesion and diversity) to produce more interpretable and well-balanced clusters.\n",
    "\n",
    "  - **Additional evaluation metrics**  \n",
    "    Incorporate metrics beyond the Silhouette Score, such as the **Davies–Bouldin index** or clustering stability scores, to gain a more comprehensive understanding of cluster quality.\n",
    "\n",
    "- **Explore automatic topic labeling**  \n",
    "  Investigate alternatives or complements to KeyBERT to reduce the reliance on manual interpretation, especially in large-scale applications.\n",
    "\n",
    "These improvements would not only increase the reliability of topic modeling results but also open the door for deeper and more scalable insights across customer-generated content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
